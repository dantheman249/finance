{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from datetime import datetime\n",
    "import lxml\n",
    "from lxml import html, etree\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from time import sleep\n",
    "import string\n",
    "import yfinance as yf\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import io\n",
    "import os\n",
    "import pandasql\n",
    "from pandasql import sqldf\n",
    "import re\n",
    "import gc\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import psutil\n",
    "\n",
    "from collections import Counter\n",
    "import linecache\n",
    "import tracemalloc\n",
    "\n",
    "import reshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top(snapshot, key_type='lineno', limit=3):\n",
    "    snapshot = snapshot.filter_traces((\n",
    "        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "        tracemalloc.Filter(False, \"<unknown>\"),\n",
    "    ))\n",
    "    top_stats = snapshot.statistics(key_type)\n",
    "\n",
    "    print(\"Top %s lines\" % limit)\n",
    "    for index, stat in enumerate(top_stats[:limit], 1):\n",
    "        frame = stat.traceback[0]\n",
    "        # replace \"/path/to/module/file.py\" with \"module/file.py\"\n",
    "        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\n",
    "        print(\"#%s: %s:%s: %.1f KiB\"\n",
    "              % (index, filename, frame.lineno, stat.size / 1024))\n",
    "        line = linecache.getline(frame.filename, frame.lineno).strip()\n",
    "        if line:\n",
    "            print('    %s' % line)\n",
    "\n",
    "    other = top_stats[limit:]\n",
    "    if other:\n",
    "        size = sum(stat.size for stat in other)\n",
    "        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\n",
    "    total = sum(stat.size for stat in top_stats)\n",
    "    print(\"Total allocated size: %.1f KiB\" % (total / 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ticker      cik\n",
      "0      aal     6201\n",
      "1      aap  1158449\n",
      "2     aapl   320193\n",
      "3     abbv  1551152\n",
      "4      abc  1140859\n",
      "..     ...      ...\n",
      "495    yum  1041061\n",
      "496    zbh  1136869\n",
      "497   zbra   877212\n",
      "498   zion   109380\n",
      "499    zts  1555280\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#get ticker, company name and CIK numbers \n",
    "ticker_url = \"https://www.sec.gov/include/ticker.txt\"\n",
    "ticker_request = requests.get(ticker_url).content\n",
    "ticker_df = pd.read_csv(io.StringIO(ticker_request.decode('utf-8')),sep=\"\\t\")\n",
    "ticker_df.columns = ['ticker', 'cik']\n",
    "ticker_df[\"ticker\"] = ticker_df[\"ticker\"].str.lower()\n",
    "\n",
    "\n",
    "#get sp500 tickers \n",
    "sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "sp500 = pd.DataFrame(sp500[0]['Symbol'])\n",
    "sp500.columns = ['ticker']\n",
    "sp500[\"ticker\"] = sp500[\"ticker\"].str.lower()\n",
    "#print(sp500)\n",
    "\n",
    "#get cik of sp 500\n",
    "sample = pd.merge(ticker_df,sp500,on='ticker')\n",
    "print(sample )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up script to traverse directory\n",
    "def generate_finance_table_csv():\n",
    "\n",
    "    #store all finances in a table\n",
    "    finance_list = []\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    for i in os.listdir():\n",
    "        file_path = i\n",
    "        #if its a quarterly filing data, parse it\n",
    "        if 'q1' in file_path or 'q2' in file_path or 'q3' in file_path or 'q4' in file_path:\n",
    "            try:\n",
    "                tracemalloc.start()\n",
    "                print(\"starting: \" + file_path)\n",
    "\n",
    "                sub = pd.read_csv(file_path + '/' + 'sub.txt', sep = '\\t', encoding = \"ISO-8859-1\", iterator = True, chunksize =10000)\n",
    "                sub = pd.concat(sub)\n",
    "                #clean up form column and filter to just 10-K submissions\n",
    "                sub['form'] = sub['form'].replace(np.nan, '', regex=True)\n",
    "                sub = sub[sub['form'] == '10-K']\n",
    "                #filter to the s&p500\n",
    "                sub = pd.merge(sub,sample, on = ['cik'])\n",
    "\n",
    "                pre = pd.read_csv(file_path + '/' + 'pre.txt', sep = '\\t', encoding = \"ISO-8859-1\", chunksize =1000000) \n",
    "                num = pd.read_csv(file_path + '/' + 'num.txt', sep = '\\t', encoding = \"ISO-8859-1\",  chunksize =100000)\n",
    "\n",
    "                print(\"joining num to sub\")\n",
    "                num_sub = pd.DataFrame()              \n",
    "                #join in numbers to submissions in chunks\n",
    "                for chunks in num: \n",
    "                    #print(chunks)\n",
    "                    num_sub = pd.concat([num_sub, sub.merge(chunks, on=['adsh'])])\n",
    "\n",
    "                #no longer need sub\n",
    "                del sub\n",
    "\n",
    "                print(\"joining pre to numsub\")\n",
    "                nsp = pd.DataFrame()\n",
    "                #add in plabel and stmt info\n",
    "                for chunks in pre:\n",
    "                    #print(chunks)\n",
    "                    nsp = pd.concat([nsp, num_sub.merge(chunks, on=['adsh', 'tag', 'version'])])\n",
    "                    nsp = nsp[nsp['stmt'].isin(['BS', 'IS', 'CF', 'CI', 'EQ'])]\n",
    "                # delete files no longer needed\n",
    "                del num_sub\n",
    "                del pre\n",
    "                del num\n",
    "\n",
    "\n",
    "                #print(nsp.columns)\n",
    "\n",
    "                nsp = nsp[['name', 'sic', 'fye', 'form', 'period', \n",
    "                           'fy', 'fp', 'filed','ticker', 'cik','ddate', \n",
    "                           'qtrs', 'uom', 'value', 'adsh','stmt',  'tag', \n",
    "                           'version', 'plabel']]\n",
    "\n",
    "                #add finances to master table\n",
    "                finance_list.append(nsp)\n",
    "                del nsp\n",
    "                print(\"processed_data: \" + file_path)\n",
    "                gc.collect()\n",
    "                snapshot = tracemalloc.take_snapshot()\n",
    "                display_top(snapshot)\n",
    "            except Exception as ex:\n",
    "                print(\"failed: \" + file_path)\n",
    "                print(str(ex))\n",
    "\n",
    "    finance_table = pd.concat(finance_list)\n",
    "    del finance_list \n",
    "    finance_table.to_csv(\"total_finance_table.csv\")\n",
    "    end = time.time()\n",
    "    print(\"finished traversing files in X secs\")\n",
    "    print(end - start)\n",
    "    #return(finance_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting: 2014q1\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2014q1\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 25078.3 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 14015.5 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: internals\\managers.py:1848: 7388.1 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "272 other: 1876.8 KiB\n",
      "Total allocated size: 48358.7 KiB\n",
      "starting: 2014q2\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2014q2\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 26137.8 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 16285.8 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: internals\\managers.py:1848: 11092.2 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "337 other: 2436.8 KiB\n",
      "Total allocated size: 55952.6 KiB\n",
      "starting: 2014q3\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2014q3\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 27837.1 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 11634.5 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: internals\\managers.py:1848: 4959.4 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "354 other: 2590.7 KiB\n",
      "Total allocated size: 47021.7 KiB\n",
      "starting: 2014q4\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2014q4\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 29581.6 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 8307.2 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: internals\\managers.py:1848: 2949.3 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "368 other: 2745.8 KiB\n",
      "Total allocated size: 43583.9 KiB\n",
      "starting: 2015q1\n",
      "joining num to sub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniel\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3254: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joining pre to numsub\n",
      "processed_data: 2015q1\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 55402.8 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 15639.8 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: internals\\managers.py:1848: 6313.8 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "395 other: 4841.8 KiB\n",
      "Total allocated size: 82198.1 KiB\n",
      "starting: 2015q2\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2015q2\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 55495.7 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 10074.6 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 3963.8 KiB\n",
      "    taken = self.values.take(indices)\n",
      "395 other: 3478.7 KiB\n",
      "Total allocated size: 73012.7 KiB\n",
      "starting: 2015q3\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2015q3\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 57183.8 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 9092.3 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 4084.3 KiB\n",
      "    taken = self.values.take(indices)\n",
      "401 other: 3032.9 KiB\n",
      "Total allocated size: 73393.4 KiB\n",
      "starting: 2015q4\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2015q4\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 59113.9 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 8360.3 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 4222.1 KiB\n",
      "    taken = self.values.take(indices)\n",
      "411 other: 2472.4 KiB\n",
      "Total allocated size: 74168.7 KiB\n",
      "starting: 2016q1\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2016q1\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 84907.6 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 13805.0 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 6064.5 KiB\n",
      "    taken = self.values.take(indices)\n",
      "417 other: 4082.4 KiB\n",
      "Total allocated size: 108859.5 KiB\n",
      "starting: 2016q2\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2016q2\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 85718.1 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 53573.3 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 40199.8 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "417 other: 7417.0 KiB\n",
      "Total allocated size: 186908.2 KiB\n",
      "starting: 2016q3\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2016q3\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 87245.8 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 52292.6 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 32747.5 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "424 other: 7549.3 KiB\n",
      "Total allocated size: 179835.2 KiB\n",
      "starting: 2016q4\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2016q4\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 89000.2 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 52132.4 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 33332.2 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "428 other: 7700.6 KiB\n",
      "Total allocated size: 182165.5 KiB\n",
      "starting: 2017q1\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2017q1\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 114072.8 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 12496.8 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 8146.7 KiB\n",
      "    taken = self.values.take(indices)\n",
      "429 other: 1892.5 KiB\n",
      "Total allocated size: 136608.9 KiB\n",
      "starting: 2017q2\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2017q2\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 115046.2 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 54597.6 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 36567.5 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "429 other: 9588.0 KiB\n",
      "Total allocated size: 215799.3 KiB\n",
      "starting: 2017q3\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2017q3\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 116554.6 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 49712.9 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 36425.1 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "435 other: 9715.9 KiB\n",
      "Total allocated size: 212408.5 KiB\n",
      "starting: 2017q4\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2017q4\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 118199.1 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 50698.6 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 38381.8 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "439 other: 9851.0 KiB\n",
      "Total allocated size: 217130.4 KiB\n",
      "starting: 2018q1\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2018q1\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 140097.3 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 52831.8 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 39093.8 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "459 other: 11428.1 KiB\n",
      "Total allocated size: 243451.0 KiB\n",
      "starting: 2018q2\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2018q2\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 141168.1 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 20639.2 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 10081.8 KiB\n",
      "    taken = self.values.take(indices)\n",
      "465 other: 4620.8 KiB\n",
      "Total allocated size: 176510.0 KiB\n",
      "starting: 2018q3\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2018q3\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 142493.7 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 50380.8 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 37908.9 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "464 other: 11627.4 KiB\n",
      "Total allocated size: 242410.9 KiB\n",
      "starting: 2018q4\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2018q4\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 144293.4 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 51361.2 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 39175.2 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "463 other: 11768.7 KiB\n",
      "Total allocated size: 246598.4 KiB\n",
      "starting: 2019q1\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2019q1\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 164941.2 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 52620.4 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 42137.5 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "464 other: 13248.6 KiB\n",
      "Total allocated size: 272947.7 KiB\n",
      "starting: 2019q2\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2019q2\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 166161.6 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 27968.6 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 11866.6 KiB\n",
      "    taken = self.values.take(indices)\n",
      "465 other: 7970.0 KiB\n",
      "Total allocated size: 213966.8 KiB\n",
      "starting: 2019q3\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2019q3\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 167616.1 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 54016.3 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 44790.6 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "464 other: 13474.5 KiB\n",
      "Total allocated size: 279897.4 KiB\n",
      "starting: 2019q4\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2019q4\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 169308.5 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 22471.8 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 12091.0 KiB\n",
      "    taken = self.values.take(indices)\n",
      "467 other: 3874.8 KiB\n",
      "Total allocated size: 207746.0 KiB\n",
      "starting: 2020q1\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2020q1\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 190639.8 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 25878.7 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 13614.7 KiB\n",
      "    taken = self.values.take(indices)\n",
      "468 other: 4506.4 KiB\n",
      "Total allocated size: 234639.5 KiB\n",
      "starting: 2020q2\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2020q2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 191545.7 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 30056.9 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 13679.2 KiB\n",
      "    taken = self.values.take(indices)\n",
      "471 other: 6831.4 KiB\n",
      "Total allocated size: 242113.2 KiB\n",
      "finished traversing files in X secs\n",
      "604.3920605182648\n"
     ]
    }
   ],
   "source": [
    "# #run function to generate finance table csv\n",
    "\n",
    "# generate_finance_table_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(df):\n",
    "    gc.collect()\n",
    "    ############################################################\n",
    "    #process data a bit \n",
    "    \n",
    "    #lower case plabel \n",
    "    df['plabel'] = df['plabel'].str.lower()\n",
    "    \n",
    "    #turn things into an integer\n",
    "    df['qtrs'] = df.qtrs.astype(int)\n",
    "    df['period'] = df.period.astype(int)\n",
    "    df['fy'] = df.fy.astype(int)\n",
    "    df['fye'] = df.fye.astype(int)\n",
    "    df['ddate'] = df.ddate.astype(int)\n",
    "    df['ddate_prev'] = df.ddate.astype(int) + 10000\n",
    "    \n",
    "    #turn things to strings\n",
    "    df['period'] = df['period'].astype(str)\n",
    "    df['fye'] = df['fye'].astype(str)\n",
    "    df['fy'] = df['fy'].astype(str)\n",
    "    df['ddate'] = df['ddate'].astype(str)\n",
    "    df['ddate_prev'] = df['ddate_prev'].astype(str)\n",
    "    #pad fye with a leading 0 \n",
    "    df['fye'] = df['fye'].apply(lambda x: x.zfill(4))\n",
    "    \n",
    "    #fill nas inplace = true changes the df directly\n",
    "    df['period'].fillna('', inplace=True)\n",
    "    df['fy'].fillna('', inplace=True)\n",
    "    df['fye'].fillna('', inplace=True)\n",
    "    df['plabel'].fillna('', inplace=True)\n",
    "    df['ddate'].fillna('', inplace=True)\n",
    "    df['ddate_prev'].fillna('', inplace=True)\n",
    "    \n",
    "    #create a joinable key\n",
    "    df['id'] = df['fy'].astype(str) + '_' + df['cik'].astype(str)\n",
    "\n",
    "    #get distinct company and years\n",
    "    key_df = pd.DataFrame(df.id.unique(), columns = ['id'])\n",
    "    gc.collect()\n",
    "    \n",
    "    ####################################################\n",
    "    #get annual revenue \n",
    "    revenue_rows = df[\n",
    "        # get yearly cash flow statements\n",
    "        (df.stmt.str.contains('IS',na=False))\n",
    "        & (df.qtrs == 4)\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('revenue',na=False) | df.plabel.str.contains('sales',na=False)) \n",
    "    ]\n",
    "    #get max value for each company and fiscal year\n",
    "    revenue_agg = revenue_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    revenue_agg.columns = ['id', 'revenue']\n",
    "    #print(revenue_agg)\n",
    "    del revenue_rows\n",
    "    \n",
    "    #get annual cash flow from operations rows \n",
    "    cfo_rows = df[\n",
    "        # get yearly cash flow statements\n",
    "        (df.stmt.str.contains('CF',na=False))\n",
    "        & (df.qtrs == 4)\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('cash',na=False)) \n",
    "        & (df.plabel.str.contains('operating',na=False)) \n",
    "        ### exclusion words #####\n",
    "        & (~df.plabel.str.contains('discontinue', na=False))\n",
    "        & (~df.plabel.str.contains('cost', na=False))\n",
    "        & (~df.plabel.str.contains('lease', na=False))\n",
    "        & (~df.plabel.str.contains('adjustments', na=False))\n",
    "        & (~df.plabel.str.contains('non-cash', na=False))\n",
    "        & (~df.plabel.str.contains('other', na=False))\n",
    "        & (~df.plabel.str.contains('disposal', na=False))\n",
    "        & (~df.plabel.str.contains('escrow', na=False))\n",
    "        & (~df.plabel.str.contains('investing', na=False))\n",
    "        & (~df.plabel.str.contains('partnership', na=False))\n",
    "        & (~df.plabel.str.contains('restricted', na=False))\n",
    "        & (~df.plabel.str.contains('securities', na=False))\n",
    "    ]\n",
    "    #get max value for each company and fiscal year\n",
    "    cfo_agg = cfo_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    cfo_agg.columns = ['id', 'cfo']\n",
    "    #print(cfo_agg)\n",
    "    del cfo_rows\n",
    "    \n",
    "    #get annual shares outstanding \n",
    "    shares_outstanding_rows = df[\n",
    "        #get yearly income stmt \n",
    "        (df.stmt.str.contains('IS',na=False))\n",
    "       # & (df.qtrs == 4)\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.uom.str.contains('shares',na=False))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('shares',na=False) | df.plabel.str.contains('stock',na=False) )\n",
    "        ### exclusion words\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    shares_outstanding_agg = shares_outstanding_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    shares_outstanding_agg.columns = ['id', 'shares_outstanding']\n",
    "    del shares_outstanding_rows\n",
    "    \n",
    "    #get current assets \n",
    "    current_assets_rows = df[\n",
    "        #get yearly balance sheet statements\n",
    "        (df.stmt.str.contains('BS',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('current',na=False)) \n",
    "        & (df.plabel.str.contains('asset',na=False)) \n",
    "    ]\n",
    "    current_assets_agg = current_assets_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    current_assets_agg.columns = ['id', 'current_assets']\n",
    "    del current_assets_rows\n",
    "    \n",
    "    #get current liabilites\n",
    "    current_liabilities_rows = df[\n",
    "        #get yearly balance sheet statements\n",
    "        (df.stmt.str.contains('BS',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('current',na=False)) \n",
    "        & (df.plabel.str.contains('liabilities',na=False)) \n",
    "    ]\n",
    "    current_liabilities_agg = current_liabilities_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    current_liabilities_agg.columns = ['id', 'current_liabilities']\n",
    "    del current_liabilities_rows\n",
    "    \n",
    "    #get depreciation and amortization\n",
    "    dep_amort_rows = df[\n",
    "        (df.stmt.str.contains('CF',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('depreciation',na=False)) \n",
    "        & (df.plabel.str.contains('amortization',na=False)) \n",
    "    ]\n",
    "    dep_amort_agg = dep_amort_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    dep_amort_agg.columns = ['id', 'dep_amort']\n",
    "    del dep_amort_rows\n",
    "    \n",
    "    # try to get depreciation only \n",
    "    dep_only_rows = df[\n",
    "        (df.stmt.str.contains('CF',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('depreciation',na=False)) \n",
    "    ]\n",
    "    dep_only_agg = dep_only_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    dep_only_agg.columns = ['id', 'dep_only']\n",
    "    del dep_only_rows\n",
    "    \n",
    "    # try to get amort only\n",
    "    amort_only_rows = df[\n",
    "        #get yearly balance sheet statements\n",
    "        (df.stmt.str.contains('CF',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('amortization',na=False)) \n",
    "    ]\n",
    "    amort_only_agg = amort_only_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    amort_only_agg.columns = ['id', 'amort_only']\n",
    "    del amort_only_rows\n",
    "    \n",
    "    #get PPE \n",
    "    ppe_rows = df[\n",
    "        #get yearly income statements\n",
    "        (df.stmt.str.contains('BS',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('property',na=False)) \n",
    "        & (df.plabel.str.contains('equipment',na=False)) \n",
    "    ]\n",
    "    ppe_agg = ppe_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    ppe_agg.columns = ['id', 'ppe']\n",
    "    del ppe_rows\n",
    "    \n",
    "    #get PPE of the year before\n",
    "    ppe_prev_rows = df[\n",
    "        (df.stmt.str.contains('BS',na=False))\n",
    "        & (df['ddate_prev'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('property',na=False)) \n",
    "        & (df.plabel.str.contains('equipment',na=False)) \n",
    "    ]\n",
    "    ppe_prev_agg = ppe_prev_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    ppe_prev_agg.columns = ['id', 'ppe_prev']\n",
    "    del ppe_prev_rows\n",
    "    \n",
    "    #get operating income \n",
    "    operating_income_rows = df[\n",
    "        #get yearly income stmt \n",
    "        (df.stmt.str.contains('IS',na=False))\n",
    "        & (df.qtrs == 4)\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('operating',na=False))\n",
    "        & (df.plabel.str.contains('income',na=False) )\n",
    "        ### exclusion words\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    operating_income_agg = operating_income_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    operating_income_agg.columns = ['id', 'operating_income']\n",
    "    del operating_income_rows\n",
    "    \n",
    "    #get long term debt on balance sheet\n",
    "    lt_debt_rows = df[\n",
    "        (df.stmt.str.contains('BS',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('debt',na=False)) \n",
    "    ]\n",
    "    lt_debt_agg = lt_debt_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    lt_debt_agg.columns = ['id', 'lt_debt']\n",
    "    del lt_debt_rows\n",
    "    \n",
    "    #get short term debt on balance sheet\n",
    "    st_debt_rows = df[\n",
    "        (df.stmt.str.contains('BS',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('st_debt',na=False)) \n",
    "    ]\n",
    "    st_debt_agg = st_debt_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    st_debt_agg.columns = ['id', 'st_debt']\n",
    "    del st_debt_rows\n",
    "    \n",
    "    #get short term debt proceeds from cash flow stmt\n",
    "    \n",
    "    #get long term debt proceeds  from cash flow stmt\n",
    "    \n",
    "    #get short term debt payments from cash flow stmt\n",
    "    \n",
    "    #get long term debt payments from cash flow stmt\n",
    "    \n",
    "    #try to get capital expenditure from cash flows \n",
    "    \n",
    "    #get cash and cash equivalents\n",
    "    cash_rows = df[\n",
    "        (df.stmt.str.contains('BS',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('cash',na=False)) \n",
    "        ### exclusion words #####\n",
    "        & (~df.plabel.str.contains('discontinue', na=False))\n",
    "        & (~df.plabel.str.contains('cost', na=False))\n",
    "        & (~df.plabel.str.contains('lease', na=False))\n",
    "        & (~df.plabel.str.contains('adjustments', na=False))\n",
    "        & (~df.plabel.str.contains('other', na=False))\n",
    "        & (~df.plabel.str.contains('disposal', na=False))\n",
    "        & (~df.plabel.str.contains('escrow', na=False))\n",
    "        & (~df.plabel.str.contains('investing', na=False))\n",
    "        & (~df.plabel.str.contains('partnership', na=False))\n",
    "        & (~df.plabel.str.contains('restricted', na=False))\n",
    "        & (~df.plabel.str.contains('securities', na=False))\n",
    "    ]\n",
    "    cash_agg = cash_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    cash_agg.columns = ['id', 'cash']\n",
    "    del cash_rows\n",
    "    \n",
    "    \n",
    "    \n",
    "    #####################################################\n",
    "    #add financials to key_df\n",
    "    merged_df = key_df.merge(cfo_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(revenue_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(shares_outstanding_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(current_assets_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(current_liabilities_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(dep_amort_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(dep_only_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(amort_only_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(ppe_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(ppe_prev_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(operating_income_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(lt_debt_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(st_debt_agg, on = ['id'], how = \"left\")\n",
    "    \n",
    "    \n",
    "    merged_df = merged_df.merge(cash_agg, on = ['id'], how = \"left\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #delete unneeded things \n",
    "    del cfo_agg\n",
    "    del shares_outstanding_agg\n",
    "    del current_assets_agg\n",
    "    del current_liabilities_agg\n",
    "    del dep_amort_agg\n",
    "    del dep_only_agg\n",
    "    del amort_only_agg\n",
    "    del ppe_agg\n",
    "    del ppe_prev_agg\n",
    "    del operating_income_agg\n",
    "    del lt_debt_agg\n",
    "    del st_debt_agg\n",
    "    del cash_agg \n",
    "    \n",
    "    \n",
    "    #print(merged_df)\n",
    "    return(merged_df)\n",
    "    #print(key_df)\n",
    "\n",
    "    \n",
    "    \n",
    "#     return(result)\n",
    "    #get all submissions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proccess the data in chunks first, return ticker level dataset \n",
    "def process_data():\n",
    "    \n",
    "    processed_list = []\n",
    "    \n",
    "    chunksize = 10 ** 5\n",
    "    for chunk in pd.read_csv(\"total_finance_table.csv\", chunksize=chunksize):  \n",
    "        #for each chunk, get finance info on a company, yearly level\n",
    "        processed_list.append(parse_data(chunk))\n",
    "    \n",
    "    #turn list of dataframes into one dataframe\n",
    "    processed_table = pd.concat(processed_list)\n",
    "    \n",
    "    #do some additional processing\n",
    "    processed_table.fillna(0, inplace = True)\n",
    "    processed_table[['year','cik']] = processed_table.id.str.split(pat = \"_\", expand=True) \n",
    "    del processed_list\n",
    "\n",
    "    #aggregate up across chunks \n",
    "    agg_df = sqldf(\n",
    "    \"\"\"\n",
    "    SELECT  \n",
    "    year,\n",
    "    CAST(cik AS INT64) as cik,\n",
    "    MAX(cfo) AS cfo, \n",
    "    MAX(shares_outstanding) AS shares_outstanding, \n",
    "    MAX(current_assets) AS current_assets,\n",
    "    MAX(current_liabilities) AS current_liabilities,\n",
    "    MAX(dep_amort) AS dep_amort,\n",
    "    MAX(dep_only) AS dep_only,\n",
    "    MAX(amort_only) AS amort_only,\n",
    "    MAX(ppe) AS ppe,\n",
    "    MAX(ppe_prev) AS ppe_prev,\n",
    "    MAX(operating_income) AS operating_income,\n",
    "    MAX(lt_debt) AS lt_debt,\n",
    "    MAX(st_debt) AS st_debt,\n",
    "    MAX(cash) AS cash \n",
    "    FROM \n",
    "    processed_table \n",
    "    GROUP BY     \n",
    "    year,\n",
    "    cik\n",
    "    ORDER BY 2 DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    del processed_table \n",
    "    \n",
    "    # turn long data wide \n",
    "    wide_df = agg_df.pivot(index='cik', columns='year', values=['cfo', 'shares_outstanding', 'current_assets',\n",
    "       'current_liabilities', 'dep_amort', 'dep_only', 'amort_only', 'ppe',\n",
    "       'ppe_prev', 'operating_income', 'lt_debt', 'st_debt', 'cash'])\n",
    "    \n",
    "    del agg_df\n",
    "    \n",
    "    #add in tickers\n",
    "    wide_df = wide_df.merge(sample, on = ['cik'])\n",
    "    return(wide_df)\n",
    "    \n",
    "start = time.time()\n",
    "\n",
    "print(\"starting to process data\")\n",
    "p_df = process_data()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_df.columns)\n",
    "print(p_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
