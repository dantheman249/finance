{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniel\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pandas_datareader\\compat\\__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import datetime as dt \n",
    "import lxml\n",
    "from lxml import html, etree\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader\n",
    "from pandas_datareader import data as pdr\n",
    "import time\n",
    "from time import sleep\n",
    "import string\n",
    "import yfinance as yf\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import io\n",
    "import os\n",
    "import pandasql\n",
    "from pandasql import sqldf\n",
    "import re\n",
    "import gc\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import psutil\n",
    "\n",
    "from collections import Counter\n",
    "import linecache\n",
    "import tracemalloc\n",
    "\n",
    "import reshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\GitHub\\\\finance'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top(snapshot, key_type='lineno', limit=3):\n",
    "    snapshot = snapshot.filter_traces((\n",
    "        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "        tracemalloc.Filter(False, \"<unknown>\"),\n",
    "    ))\n",
    "    top_stats = snapshot.statistics(key_type)\n",
    "\n",
    "    print(\"Top %s lines\" % limit)\n",
    "    for index, stat in enumerate(top_stats[:limit], 1):\n",
    "        frame = stat.traceback[0]\n",
    "        # replace \"/path/to/module/file.py\" with \"module/file.py\"\n",
    "        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\n",
    "        print(\"#%s: %s:%s: %.1f KiB\"\n",
    "              % (index, filename, frame.lineno, stat.size / 1024))\n",
    "        line = linecache.getline(frame.filename, frame.lineno).strip()\n",
    "        if line:\n",
    "            print('    %s' % line)\n",
    "\n",
    "    other = top_stats[limit:]\n",
    "    if other:\n",
    "        size = sum(stat.size for stat in other)\n",
    "        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\n",
    "    total = sum(stat.size for stat in top_stats)\n",
    "    print(\"Total allocated size: %.1f KiB\" % (total / 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ticker      cik\n",
      "0      aal     6201\n",
      "1      aap  1158449\n",
      "2     aapl   320193\n",
      "3     abbv  1551152\n",
      "4      abc  1140859\n",
      "..     ...      ...\n",
      "495    yum  1041061\n",
      "496    zbh  1136869\n",
      "497   zbra   877212\n",
      "498   zion   109380\n",
      "499    zts  1555280\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#get ticker, company name and CIK numbers \n",
    "ticker_url = \"https://www.sec.gov/include/ticker.txt\"\n",
    "ticker_request = requests.get(ticker_url).content\n",
    "ticker_df = pd.read_csv(io.StringIO(ticker_request.decode('utf-8')),sep=\"\\t\")\n",
    "ticker_df.columns = ['ticker', 'cik']\n",
    "ticker_df[\"ticker\"] = ticker_df[\"ticker\"].str.lower()\n",
    "\n",
    "\n",
    "#get sp500 tickers \n",
    "sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "sp500 = pd.DataFrame(sp500[0]['Symbol'])\n",
    "sp500.columns = ['ticker']\n",
    "sp500[\"ticker\"] = sp500[\"ticker\"].str.lower()\n",
    "#print(sp500)\n",
    "\n",
    "#get cik of sp 500\n",
    "sample = pd.merge(ticker_df,sp500,on='ticker')\n",
    "print(sample )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get tickers in the sp 500 and get easy stats on them\n",
    "#get avg price by year, and current price of stock \n",
    "def get_easy_stats(df):\n",
    "    \n",
    "    easy_df = []\n",
    "    \n",
    "    start_dt = dt.datetime(2013,1,1)\n",
    "    end_dt = dt.datetime(2019,12,31)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        try: \n",
    "        \n",
    "            sleep(0.2)\n",
    "            #data_ts = pdr.get_data_yahoo(row['ticker'], start_dt, end_dt)\n",
    "            data_ts = yf.download(row['ticker'], start= start_dt, end= end_dt, progress=False)\n",
    "            #print(data_ts)\n",
    "                                      \n",
    "            data_ts['year'] = data_ts.index.year\n",
    "            #get average price per share \n",
    "            agg_df = data_ts.groupby('year', as_index=False)['Adj Close'].mean()\n",
    "            agg_df['ticker'] = row['ticker']\n",
    "            agg_df.columns = ['year','avg_price', 'ticker']\n",
    "            agg_df['year']=  agg_df['year'].astype(str)\n",
    "    #       print(agg_df)\n",
    "            current_price = pdr.get_data_yahoo(row['ticker']).last('1D')['Adj Close'][0].astype(int)\n",
    "\n",
    "            #turn agg_df wide  for price rows\n",
    "            wide_df = agg_df.pivot(index='ticker', columns='year', values=['avg_price'])\n",
    "\n",
    "            wide_df.columns = list(map(\"\".join, wide_df.columns))\n",
    "            wide_df['current_price'] = current_price\n",
    "            print(wide_df)\n",
    "\n",
    "            easy_df.append(wide_df)\n",
    "            del agg_df\n",
    "            del wide_df\n",
    "        \n",
    "        except Exception as ex:\n",
    "            print(row['ticker'] + \" failed\")\n",
    "            print(ex)\n",
    "            \n",
    "    \n",
    "    easy_df = pd.concat(easy_df)\n",
    "    #easy_df.to_csv(\"easy_stats_sp500.csv\")\n",
    "    return(easy_df)\n",
    "#function end\n",
    "\n",
    "start = time.time()\n",
    "#run to generate easy stat csv\n",
    "easy_stat_df = get_easy_stats(sample)\n",
    "easy_stat_df.to_csv(\"easy_stats_sp500.csv\")\n",
    "print(easy_stat_df)\n",
    "\n",
    "end = time.time()\n",
    "print(\"finished getting easy stats for sp500\")\n",
    "print(end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ticker  avg_price2013  avg_price2014  avg_price2015  avg_price2016  \\\n",
      "0      aal      17.146522      36.755626      42.940213      36.716199   \n",
      "1      aap      83.696200     128.910822     161.370373     153.556964   \n",
      "2     aapl      59.771992      83.652771     110.696663      98.462125   \n",
      "3     abbv      32.361064      42.512583      49.072086      49.757348   \n",
      "4      abc      50.829051      67.690750      95.513013      77.419414   \n",
      "..     ...            ...            ...            ...            ...   \n",
      "465    yum      43.882041      47.608164      52.553995      55.586190   \n",
      "466    zbh      75.714952      96.441867     104.284561     109.899370   \n",
      "467   zbra      46.445833      71.196667      89.878492      64.032024   \n",
      "468   zion      24.429700      26.550984      26.060497      26.514911   \n",
      "469    zts      30.203858      32.603061      44.496182      46.556359   \n",
      "\n",
      "     avg_price2017  avg_price2018  avg_price2019  current_price  \n",
      "0        46.100999      41.898407      30.579955             11  \n",
      "1       121.234932     138.043532     157.399114            147  \n",
      "2       144.384174     184.072185     205.625419            370  \n",
      "3        64.069739      85.806370      72.289784             97  \n",
      "4        81.515308      85.814323      81.267888            103  \n",
      "..             ...            ...            ...            ...  \n",
      "465      68.758763      81.697489     102.408614             93  \n",
      "466     115.598098     115.463559     127.191131            134  \n",
      "467      99.310319     150.661314     206.514342            269  \n",
      "468      41.197410      49.195448      44.897323             33  \n",
      "469      59.965932      84.415576     109.556135            144  \n",
      "\n",
      "[470 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "#read in the easy stats of sp500\n",
    "#easy_stat_df.to_csv(\"easy_stats_sp500.csv\")\n",
    "easy_stat_df = pd.read_csv(\"easy_stats_sp500.csv\")\n",
    "print(easy_stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up script to traverse directory of quarterly submissions\n",
    "def generate_finance_table_csv():\n",
    "\n",
    "    #store all finances in a table\n",
    "    finance_list = []\n",
    "\n",
    "    #go into directory with data\n",
    "    file_os = 'D:\\\\Finance Data'\n",
    "    \n",
    "    \n",
    "    code_os = 'D:\\\\GitHub\\\\finance'\n",
    "    start = time.time()\n",
    "    \n",
    "    #go into file directory\n",
    "    os.chdir(file_os)\n",
    "    \n",
    "    for i in os.listdir():\n",
    "        file_path = i\n",
    "        #if its a quarterly filing data, parse it\n",
    "        if 'q1' in file_path or 'q2' in file_path or 'q3' in file_path or 'q4' in file_path:\n",
    "            try:\n",
    "                tracemalloc.start()\n",
    "                print(\"starting: \" + file_path)\n",
    "\n",
    "                sub = pd.read_csv(file_path + '/' + 'sub.txt', sep = '\\t', encoding = \"ISO-8859-1\", iterator = True, chunksize =100000)\n",
    "                sub = pd.concat(sub)\n",
    "                #clean up form column and filter to just 10-K submissions\n",
    "                sub['form'] = sub['form'].replace(np.nan, '', regex=True)\n",
    "                sub = sub[sub['form'] == '10-K']\n",
    "                #filter to the s&p500\n",
    "                sub = pd.merge(sub,sample, on = ['cik'])\n",
    "\n",
    "                pre = pd.read_csv(file_path + '/' + 'pre.txt', sep = '\\t', encoding = \"ISO-8859-1\", chunksize =1000000) \n",
    "                num = pd.read_csv(file_path + '/' + 'num.txt', sep = '\\t', encoding = \"ISO-8859-1\",  chunksize =100000)\n",
    "\n",
    "                print(\"joining num to sub\")\n",
    "                num_sub = pd.DataFrame()              \n",
    "                #join in numbers to submissions in chunks\n",
    "                for chunks in num: \n",
    "                    #print(chunks)\n",
    "                    num_sub = pd.concat([num_sub, sub.merge(chunks, on=['adsh'])])\n",
    "\n",
    "                #no longer need sub\n",
    "                del sub\n",
    "\n",
    "                print(\"joining pre to numsub\")\n",
    "                nsp = pd.DataFrame()\n",
    "                #add in plabel and stmt info\n",
    "                for chunks in pre:\n",
    "                    #print(chunks)\n",
    "                    nsp = pd.concat([nsp, num_sub.merge(chunks, on=['adsh', 'tag', 'version'])])\n",
    "                    nsp = nsp[nsp['stmt'].isin(['BS', 'IS', 'CF', 'CI', 'EQ'])]\n",
    "                # delete files no longer needed\n",
    "                del num_sub\n",
    "                del pre\n",
    "                del num\n",
    "\n",
    "\n",
    "                #print(nsp.columns)\n",
    "\n",
    "                nsp = nsp[['name', 'sic', 'fye', 'form', 'period', \n",
    "                           'fy', 'fp', 'filed','ticker', 'cik','ddate', \n",
    "                           'qtrs', 'uom', 'value', 'adsh','stmt',  'tag', \n",
    "                           'version', 'plabel']]\n",
    "\n",
    "                #add finances to master table\n",
    "                finance_list.append(nsp)\n",
    "                del nsp\n",
    "                print(\"processed_data: \" + file_path)\n",
    "                gc.collect()\n",
    "                snapshot = tracemalloc.take_snapshot()\n",
    "                display_top(snapshot)\n",
    "            except Exception as ex:\n",
    "                print(\"failed: \" + file_path)\n",
    "                print(str(ex))\n",
    "    \n",
    "    #switch back to github directory \n",
    "    os.chdir(code_os)\n",
    "    \n",
    "    \n",
    "    finance_table = pd.concat(finance_list)\n",
    "    del finance_list \n",
    "    finance_table.to_csv(\"total_finance_table.csv\")\n",
    "    end = time.time()\n",
    "    print(\"finished traversing files in X secs\")\n",
    "    print(end - start)\n",
    "    #return(finance_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run function to generate finance table csv\n",
    "generate_finance_table_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function goes through and tries to parse out the historical data we need: VERY LONG \n",
    "def parse_data(df):\n",
    "    gc.collect()\n",
    "    ############################################################\n",
    "    #process data a bit \n",
    "    \n",
    "    #lower case plabel \n",
    "    df['plabel'] = df['plabel'].str.lower()\n",
    "    \n",
    "    #turn things into an integer\n",
    "    df['qtrs'] = df.qtrs.astype(int)\n",
    "    df['period'] = df.period.astype(int)\n",
    "    df['fy'] = df.fy.astype(int)\n",
    "    df['fye'] = df.fye.astype(int)\n",
    "    df['ddate'] = df.ddate.astype(int)\n",
    "    df['ddate_prev'] = df.ddate.astype(int) + 10000\n",
    "    \n",
    "    #turn things to strings\n",
    "    df['period'] = df['period'].astype(str)\n",
    "    df['fye'] = df['fye'].astype(str)\n",
    "    df['fy'] = df['fy'].astype(str)\n",
    "    df['ddate'] = df['ddate'].astype(str)\n",
    "    df['ddate_prev'] = df['ddate_prev'].astype(str)\n",
    "    #pad fye with a leading 0 \n",
    "    df['fye'] = df['fye'].apply(lambda x: x.zfill(4))\n",
    "    \n",
    "    #fill nas inplace = true changes the df directly\n",
    "    df['period'].fillna('', inplace=True)\n",
    "    df['fy'].fillna('', inplace=True)\n",
    "    df['fye'].fillna('', inplace=True)\n",
    "    df['plabel'].fillna('', inplace=True)\n",
    "    df['ddate'].fillna('', inplace=True)\n",
    "    df['ddate_prev'].fillna('', inplace=True)\n",
    "    \n",
    "    #create a joinable key\n",
    "    df['id'] = df['fy'].astype(str) + '_' + df['cik'].astype(str)\n",
    "\n",
    "    #get distinct company and years\n",
    "    key_df = pd.DataFrame(df.id.unique(), columns = ['id'])\n",
    "    gc.collect()\n",
    "    \n",
    "    #######################################################################################################\n",
    "    #define tags\n",
    "    \n",
    "    #for shares outstanding calc (all qtrs = 4, income stmt)\n",
    "    shares_outstanding_tags = ['WeightedAverageNumberOfDilutedSharesOutstanding']\n",
    "    net_income_tags = ['NetIncomeLoss']\n",
    "    eps_tags = ['EarningsPerShareDiluted']\n",
    "    \n",
    "    # for fcfe calculation  (cash flow stmt)\n",
    "    cfo_tags = ['NetCashProvidedByUsedInOperatingActivities']\n",
    "    debt_repayments_tags= ['RepaymentsOfDebt']\n",
    "    debt_proceeds_tags = ['ProceedsFromIssuanceOfDebt']\n",
    "    \n",
    "    #for capital expenditure calculations (cash flow statement)\n",
    "    capex_tags = ['PaymentsToAcquireProductiveAssets']\n",
    "    ppe_purchase_tags = ['PaymentsToAcquirePropertyPlantAndEquipment']\n",
    "    ppe_sale_tags = ['ProceedsFromSaleOfPropertyPlantAndEquipment']\n",
    "    #qtrs = 0; for ppe on balance sheet\n",
    "    ppe_tags = ['PropertyPlantAndEquipmentNet']\n",
    "\n",
    "    #for ebitda calculation\n",
    "    revenue_tags = ['Revenues']\n",
    "    operating_income_tags = ['OperatingIncomeLoss']\n",
    "    dep_amort_tags = ['DepreciationDepletionAndAmortization']\n",
    "    \n",
    "    #######################################################################################################\n",
    "    #pull rows\n",
    "    \n",
    "#shares outstanding\n",
    "    shares_outstanding_rows = df[\n",
    "        (df['tag'].isin(shares_outstanding_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.uom.str.contains('shares',na=False))\n",
    "    ]\n",
    "\n",
    "    shares_outstanding_agg = shares_outstanding_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    shares_outstanding_agg.columns = ['id', 'shares_outstanding']\n",
    "    del shares_outstanding_rows\n",
    "#net income\n",
    "    net_income_rows = df[\n",
    "        (df['tag'].isin(net_income_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    net_income_agg = net_income_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    net_income_agg.columns = ['id', 'net_income']\n",
    "    del net_income_rows\n",
    "#eps\n",
    "    eps_rows = df[\n",
    "        (df['tag'].isin(eps_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    eps_agg = eps_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    eps_agg.columns = ['id', 'eps']\n",
    "    del eps_rows\n",
    "#cash flow from operations    \n",
    "    cfo_rows = df[\n",
    "        (df['tag'].isin(cfo_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    cfo_agg = cfo_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    cfo_agg.columns = ['id', 'cfo']\n",
    "    del cfo_rows\n",
    "#debt repayment \n",
    "    debt_repayments_rows = df[\n",
    "        (df['tag'].isin(debt_repayments_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    debt_repayments_agg = debt_repayments_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    debt_repayments_agg.columns = ['id', 'debt_repayments']\n",
    "    del debt_repayments_rows\n",
    "#debt proceeds \n",
    "    debt_proceeds_rows = df[\n",
    "        (df['tag'].isin(debt_proceeds_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    debt_proceeds_agg = debt_proceeds_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    debt_proceeds_agg.columns = ['id', 'debt_proceeds']\n",
    "    del debt_proceeds_rows\n",
    "#cap ex\n",
    "    capex_rows = df[\n",
    "        (df['tag'].isin(capex_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    capex_agg = capex_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    capex_agg.columns = ['id', 'capex']\n",
    "    del capex_rows\n",
    "#ppe purchase\n",
    "    ppe_purchase_rows = df[\n",
    "        (df['tag'].isin(ppe_purchase_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    ppe_purchase_agg = ppe_purchase_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    ppe_purchase_agg.columns = ['id', 'ppe_purchase']\n",
    "    del ppe_purchase_rows\n",
    "#ppe sale \n",
    "    ppe_sale_rows = df[\n",
    "        (df['tag'].isin(ppe_sale_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    ppe_sale_agg = ppe_sale_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    ppe_sale_agg.columns = ['id', 'ppe_sale']\n",
    "    del ppe_sale_rows\n",
    "#ppe , qtrs = 0; for ppe on balance sheet\n",
    "    ppe_rows = df[\n",
    "        (df['tag'].isin(ppe_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 0)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    ppe_agg = ppe_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    ppe_agg.columns = ['id', 'ppe']\n",
    "    del ppe_rows\n",
    "#get PPE of the year before\n",
    "    ppe_prev_rows = df[\n",
    "        (df['tag'].isin(ppe_tags))\n",
    "        & (df['ddate_prev'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 0)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    ppe_prev_agg = ppe_prev_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    ppe_prev_agg.columns = ['id', 'ppe_prev']\n",
    "    del ppe_prev_rows\n",
    "#revenue \n",
    "    revenue_rows = df[\n",
    "        (df['tag'].isin(revenue_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    revenue_agg = revenue_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    revenue_agg.columns = ['id', 'revenue']\n",
    "    del revenue_rows\n",
    "#operating income\n",
    "    operating_income_rows = df[\n",
    "        (df['tag'].isin(operating_income_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    operating_income_agg = operating_income_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    operating_income_agg.columns = ['id', 'operating_income']\n",
    "    del operating_income_rows\n",
    "#depreciation and amortization\n",
    "    dep_amort_rows = df[\n",
    "        (df['tag'].isin(dep_amort_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    dep_amort_agg = dep_amort_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    dep_amort_agg.columns = ['id', 'dep_amort']\n",
    "    del dep_amort_rows\n",
    "\n",
    "\n",
    "#     #get current assets \n",
    "#     #get current liabilites\n",
    "#     #get long term debt on balance sheet\n",
    "#     #get short term debt on balance sheet \n",
    "#     #get cash and cash equivalents\n",
    "\n",
    "#     #######################################################################################################\n",
    "    #add financials to key_df\n",
    "    merged_df = key_df.merge(shares_outstanding_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(net_income_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(eps_agg, on = ['id'], how = \"left\")\n",
    "    \n",
    "    merged_df = merged_df.merge(cfo_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(debt_proceeds_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(debt_repayments_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(capex_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(ppe_purchase_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(ppe_sale_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(ppe_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(ppe_prev_agg, on = ['id'], how = \"left\")\n",
    "    \n",
    "    merged_df = merged_df.merge(revenue_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(operating_income_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(dep_amort_agg, on = ['id'], how = \"left\")\n",
    "    \n",
    "    \n",
    "#     #######################################################################################################\n",
    "#     #delete unneeded things \n",
    "    del shares_outstanding_agg\n",
    "    del net_income_agg\n",
    "    del eps_agg\n",
    "    del cfo_agg\n",
    "    del debt_proceeds_agg\n",
    "    del debt_repayments_agg\n",
    "    del capex_agg\n",
    "    del ppe_purchase_agg\n",
    "    del ppe_sale_agg\n",
    "    del ppe_agg\n",
    "    del ppe_prev_agg\n",
    "    del revenue_agg\n",
    "    del operating_income_agg\n",
    "    del dep_amort_agg\n",
    "    return(merged_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to process data\n",
      "printing pre-aggregated data\n",
      "               id  shares_outstanding    net_income    eps           cfo  \\\n",
      "0       2013_4904        4.870410e+08  1.480000e+09   3.04  4.106000e+09   \n",
      "1       2013_4977        4.674080e+08  3.158000e+09   6.76  1.054700e+10   \n",
      "2       2013_5513                 NaN  8.581000e+08   3.23  1.031500e+09   \n",
      "3       2013_6201        2.763260e+08 -1.526000e+09 -11.25  9.490000e+08   \n",
      "4       2013_7084        6.630000e+08  1.342000e+09   2.02  5.226000e+09   \n",
      "..            ...                 ...           ...    ...           ...   \n",
      "414  2019_1688568                 NaN  1.257000e+09   4.47  1.783000e+09   \n",
      "415  2019_1757898                 NaN  3.040510e+08   3.56  5.395050e+08   \n",
      "416    2019_56873        8.180000e+08  3.110000e+09   3.76  4.164000e+09   \n",
      "417   2019_849399        6.320000e+08  3.100000e+07   0.05  1.495000e+09   \n",
      "418  2020_1613103        1.351100e+09  4.789000e+09   3.54  7.234000e+09   \n",
      "\n",
      "     debt_proceeds  debt_repayments         capex  ppe_purchase     ppe_sale  \\\n",
      "0              NaN              NaN           NaN  3.200000e+07   62975000.0   \n",
      "1              NaN              NaN           NaN           NaN          NaN   \n",
      "2              NaN              NaN           NaN  1.055000e+08          NaN   \n",
      "3              NaN              NaN  3.114000e+09           NaN          NaN   \n",
      "4              NaN              NaN           NaN  9.130000e+08          NaN   \n",
      "..             ...              ...           ...           ...          ...   \n",
      "414            NaN              NaN           NaN  2.970000e+08  357000000.0   \n",
      "415            NaN              NaN           NaN  1.897150e+08    5567000.0   \n",
      "416            NaN              NaN  2.967000e+09           NaN          NaN   \n",
      "417            NaN              NaN           NaN  2.070000e+08   26000000.0   \n",
      "418            NaN              NaN           NaN  1.213000e+09          NaN   \n",
      "\n",
      "              ppe      ppe_prev       revenue  operating_income     dep_amort  \\\n",
      "0    4.099700e+10  3.876300e+10  1.535700e+10      2.855000e+09           NaN   \n",
      "1    4.810000e+08  5.640000e+08  2.393900e+10               NaN           NaN   \n",
      "2    5.119000e+08  5.016000e+08  1.035380e+10               NaN           NaN   \n",
      "3    1.925900e+10  1.340200e+10           NaN      1.534000e+09  1.020000e+09   \n",
      "4    1.013700e+10  1.012300e+10  8.980400e+10               NaN  9.090000e+08   \n",
      "..            ...           ...           ...               ...           ...   \n",
      "414  3.179000e+09           NaN  2.075300e+10               NaN  2.023000e+09   \n",
      "415  1.031582e+09           NaN  2.782170e+09      4.114650e+08  2.259210e+08   \n",
      "416  2.163500e+10           NaN           NaN      2.614000e+09  2.465000e+09   \n",
      "417  6.630000e+08           NaN           NaN      1.580000e+08  6.150000e+08   \n",
      "418  4.828000e+09  4.675000e+09           NaN      4.791000e+09  2.663000e+09   \n",
      "\n",
      "     year      cik  \n",
      "0    2013     4904  \n",
      "1    2013     4977  \n",
      "2    2013     5513  \n",
      "3    2013     6201  \n",
      "4    2013     7084  \n",
      "..    ...      ...  \n",
      "414  2019  1688568  \n",
      "415  2019  1757898  \n",
      "416  2019    56873  \n",
      "417  2019   849399  \n",
      "418  2020  1613103  \n",
      "\n",
      "[7444 rows x 17 columns]\n",
      "Index(['year', 'cik', 'shares_outstanding', 'net_income', 'eps', 'cfo',\n",
      "       'debt_proceeds', 'debt_repayments', 'capex', 'ppe_purchase', 'ppe_sale',\n",
      "       'ppe', 'ppe_prev', 'revenue', 'operating_income', 'dep_amort', 'ebitda',\n",
      "       'shares_outstanding_coalesced', 'capex_coalesced', 'net_debt_proceeds',\n",
      "       'fcfe', 'fcfe_modded'],\n",
      "      dtype='object')\n",
      "printing df to write to csv\n",
      "         cik  shares_outstanding2013  shares_outstanding2014  \\\n",
      "0       1800            1.574000e+09            1.527000e+09   \n",
      "1       2488            7.660000e+08            7.760000e+08   \n",
      "2       2969                     NaN                     NaN   \n",
      "3       4127                     NaN                     NaN   \n",
      "4       4281                     NaN                     NaN   \n",
      "..       ...                     ...                     ...   \n",
      "465  1751788                     NaN                     NaN   \n",
      "466  1754301                     NaN                     NaN   \n",
      "467  1754301                     NaN                     NaN   \n",
      "468  1755672                     NaN                     NaN   \n",
      "469  1757898                     NaN                     NaN   \n",
      "\n",
      "     shares_outstanding2015  shares_outstanding2016  shares_outstanding2017  \\\n",
      "0              1.506000e+09            1.483000e+09            1.749000e+09   \n",
      "1              7.910000e+08            9.310000e+08            1.039000e+09   \n",
      "2              2.173000e+08            2.183000e+08            2.198000e+08   \n",
      "3              1.949000e+08            1.921000e+08            1.867000e+08   \n",
      "4                       NaN                     NaN                     NaN   \n",
      "..                      ...                     ...                     ...   \n",
      "465                     NaN                     NaN                     NaN   \n",
      "466                     NaN                     NaN                     NaN   \n",
      "467                     NaN                     NaN                     NaN   \n",
      "468                     NaN                     NaN                     NaN   \n",
      "469                     NaN                     NaN                     NaN   \n",
      "\n",
      "     shares_outstanding2018  shares_outstanding2019  net_income2013  \\\n",
      "0              1.770000e+09            1.781000e+09    2.576000e+09   \n",
      "1              1.079000e+09            1.188000e+09   -8.300000e+07   \n",
      "2              2.209000e+08            2.221000e+08             NaN   \n",
      "3              1.832000e+08            1.745000e+08             NaN   \n",
      "4                       NaN            4.630000e+08   -2.285000e+09   \n",
      "..                      ...                     ...             ...   \n",
      "465                     NaN            7.425000e+08             NaN   \n",
      "466                     NaN                     NaN             NaN   \n",
      "467                     NaN                     NaN             NaN   \n",
      "468                     NaN                     NaN             NaN   \n",
      "469                     NaN                     NaN             NaN   \n",
      "\n",
      "     net_income2014  ...  fcfe_modded2019  ticker  avg_price2013  \\\n",
      "0      2.284000e+09  ...     4.498000e+09     abt      30.865046   \n",
      "1     -4.030000e+08  ...     2.760000e+08     amd       3.378571   \n",
      "2      9.917000e+08  ...              NaN     apd      74.559651   \n",
      "3      4.577000e+08  ...     9.690000e+08    swks      21.678093   \n",
      "4      2.680000e+08  ...    -1.800000e+08     hwm            NaN   \n",
      "..              ...  ...              ...     ...            ...   \n",
      "465             NaN  ...     4.116000e+09     dow            NaN   \n",
      "466             NaN  ...     2.289000e+09     fox            NaN   \n",
      "467             NaN  ...     2.289000e+09    foxa            NaN   \n",
      "468             NaN  ...    -9.300000e+07    ctva            NaN   \n",
      "469             NaN  ...     3.553570e+08     ste      38.724741   \n",
      "\n",
      "     avg_price2014  avg_price2015  avg_price2016  avg_price2017  \\\n",
      "0        36.177461      41.761090      37.389035      45.955036   \n",
      "1         3.662064       2.331746       5.254921      12.411673   \n",
      "2        99.016036     114.534238     119.110206     135.804867   \n",
      "3        43.898232      82.857883      66.311559      95.660652   \n",
      "4              NaN            NaN            NaN            NaN   \n",
      "..             ...            ...            ...            ...   \n",
      "465            NaN            NaN            NaN            NaN   \n",
      "466            NaN            NaN            NaN            NaN   \n",
      "467            NaN            NaN            NaN            NaN   \n",
      "468            NaN            NaN            NaN            NaN   \n",
      "469      49.605464      63.724561      65.699793      77.171300   \n",
      "\n",
      "     avg_price2018  avg_price2019  current_price  \n",
      "0        61.982924      79.042221             98  \n",
      "1        17.214502      29.877649             69  \n",
      "2       154.404748     202.752672            291  \n",
      "3        89.492518      82.058670            131  \n",
      "4              NaN      29.213175             15  \n",
      "..             ...            ...            ...  \n",
      "465            NaN      47.501288             42  \n",
      "466            NaN      34.544853             26  \n",
      "467            NaN      34.978007             26  \n",
      "468            NaN      27.161469             28  \n",
      "469     102.022221     135.685754            156  \n",
      "\n",
      "[470 rows x 150 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing data\n",
      "29.261815309524536\n"
     ]
    }
   ],
   "source": [
    "# process the data in chunks first, return ticker level dataset \n",
    "def init_process_data():\n",
    "    \n",
    "    processed_list = []\n",
    "    \n",
    "    chunksize = 10 ** 5\n",
    "    for chunk in pd.read_csv(\"total_finance_table.csv\", chunksize=chunksize):  \n",
    "        #for each chunk, get finance info on a company, yearly level\n",
    "        processed_list.append(parse_data(chunk))\n",
    "    #turn list of dataframes into one dataframe\n",
    "    processed_table = pd.concat(processed_list)\n",
    "    \n",
    "\n",
    "    #split id back into year and cik\n",
    "    processed_table[['year','cik']] = processed_table.id.str.split(pat = \"_\", expand=True) \n",
    "    print(\"printing pre-aggregated data\")\n",
    "    print(processed_table)\n",
    "    del processed_list\n",
    "    #######################################################################################################\n",
    "    #aggregate up across chunks \n",
    "    agg_df = sqldf(\n",
    "    \"\"\"\n",
    "     SELECT\n",
    "    *, \n",
    "    cfo - capex_coalesced + net_debt_proceeds AS fcfe, \n",
    "    cfo - capex_coalesced AS fcfe_modded\n",
    "    FROM \n",
    "    (\n",
    "         SELECT \n",
    "        *,\n",
    "        IFNULL(operating_income,0) + IFNULL(dep_amort,0) AS ebitda, \n",
    "\n",
    "        CASE WHEN shares_outstanding IS NULL THEN net_income/NULLIF(eps,0) \n",
    "             ELSE shares_outstanding \n",
    "             END AS shares_outstanding_coalesced, \n",
    "\n",
    "        CASE WHEN capex IS NOT NULL THEN capex \n",
    "             WHEN ppe_purchase IS NOT NULL THEN ppe_purchase - IFNULL(ppe_sale,0)\n",
    "             WHEN ppe IS NOT NULL THEN ppe - IFNULL(ppe_prev,0) \n",
    "             END AS capex_coalesced, \n",
    "             \n",
    "        IFNULL(debt_proceeds,0) - IFNULL(debt_repayments,0) AS net_debt_proceeds\n",
    "        \n",
    "        FROM \n",
    "        (\n",
    "            SELECT  \n",
    "            year,\n",
    "            CAST(cik AS INT64) as cik,\n",
    "            MAX(shares_outstanding) AS shares_outstanding,\n",
    "            MAX(net_income) AS net_income,\n",
    "            MAX(eps) AS eps,\n",
    "            MAX(cfo) AS cfo,\n",
    "            MAX(debt_proceeds) AS debt_proceeds,\n",
    "            MAX(debt_repayments) AS debt_repayments,\n",
    "            MAX(capex) AS capex,\n",
    "            MAX(ppe_purchase) AS ppe_purchase,\n",
    "            MAX(ppe_sale) AS ppe_sale,\n",
    "            MAX(ppe) AS ppe,\n",
    "            MAX(ppe_prev) AS ppe_prev, \n",
    "            MAX(revenue) AS revenue,\n",
    "            MAX(operating_income) AS operating_income,\n",
    "            MAX(dep_amort) AS dep_amort\n",
    "            FROM \n",
    "            processed_table \n",
    "            GROUP BY     \n",
    "            year,\n",
    "            cik\n",
    "            ORDER BY 2 DESC\n",
    "        )\n",
    "        )\n",
    "            \n",
    "    \"\"\")\n",
    "    \n",
    "    del processed_table \n",
    "    print(agg_df.columns)\n",
    "    \n",
    "#     #######################################################################################################\n",
    "    # turn long data wide \n",
    "    wide_df = agg_df.pivot(index='cik', columns='year', values=['shares_outstanding', 'net_income', 'eps', 'cfo',\n",
    "       'debt_proceeds', 'debt_repayments', 'capex', 'ppe_purchase', 'ppe_sale',\n",
    "       'ppe', 'ppe_prev', 'revenue', 'operating_income', 'dep_amort', 'ebitda',\n",
    "       'shares_outstanding_coalesced', 'capex_coalesced', 'net_debt_proceeds',\n",
    "       'fcfe', 'fcfe_modded'])\n",
    "    \n",
    "    del agg_df\n",
    "    \n",
    "#     ####################################################################################################### \n",
    "    #fix columns\n",
    "    wide_df.columns = list(map(\"\".join, wide_df.columns))\n",
    "    \n",
    "    #add in tickers\n",
    "    wide_df = wide_df.merge(sample, on = ['cik'])\n",
    "    \n",
    "    #drop weird columns \n",
    "    wide_df = wide_df[wide_df.columns.drop(list(wide_df.filter(regex='1218')))]\n",
    "    wide_df = wide_df[wide_df.columns.drop(list(wide_df.filter(regex='2020')))]\n",
    "    \n",
    "    #read in easy stats csv\n",
    "    easy_stat_df = pd.read_csv(\"easy_stats_sp500.csv\")\n",
    "    \n",
    "    #join in easy stats\n",
    "    wide_df = wide_df.merge(easy_stat_df, on = 'ticker')\n",
    "    \n",
    "#     ####################################################################################################### \n",
    "#     # add dataquality columns - \n",
    "#     #WTK: how many revenue, ebitda, fcfe and shares_outstanding columns are available with 2019 info available\n",
    "    \n",
    "#     # WTK: calculated estimated cagrs (FCFE growth, Revenue growth) and multiples (fcfe yield, EV/EBITDA)\n",
    "    \n",
    "#     # forecast earnings, growth, multiple expansion or decrease over 10 years and get estimated IRR of stock\n",
    "    \n",
    "    \n",
    "#     ####################################################################################################### \n",
    "    #write to csv\n",
    "    print(\"printing df to write to csv\")\n",
    "    print(wide_df)\n",
    "    wide_df.to_csv(\"sp500_financials.csv\")\n",
    "    return(wide_df)\n",
    "    \n",
    "start = time.time()\n",
    "\n",
    "print(\"starting to process data\")\n",
    "init_df = init_process_data()\n",
    "\n",
    "end = time.time()\n",
    "print(\"finished processing data\")\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in init_df.columns: print(i)\n",
    "#print(init_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ticker\n",
      "0      abt\n",
      "1      amd\n",
      "2      apd\n",
      "3     swks\n",
      "4      hwm\n",
      "..     ...\n",
      "441    dis\n",
      "442   amcr\n",
      "443    dow\n",
      "444   ctva\n",
      "445    ste\n",
      "\n",
      "[446 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#assess quality of data here \n",
    "pt_good = sqldf(\"\"\"SELECT DISTINCT ticker FROM init_df \n",
    "                    WHERE shares_outstanding_coalesced2019 > 0\n",
    "                \"\"\")\n",
    "print(pt_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get other ticker info from other script (current price, dividend yield, market cap, shares outsanding , sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 2018 walmart ticker = wmt, fy = 2018\n",
    "# analyze missing info \n",
    "dq_df = pd.read_csv(\"total_finance_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = dq_df[(dq_df['ticker'] == 'dis')\n",
    "           #& (dq_df['fy'] == 2015.0)\n",
    "           \n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in ex.iterrows():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
