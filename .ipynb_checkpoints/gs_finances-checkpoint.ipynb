{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import lxml\n",
    "from lxml import html, etree\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from time import sleep\n",
    "import string\n",
    "import yfinance as yf\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import io\n",
    "import os\n",
    "import pandasql\n",
    "from pandasql import sqldf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first make a function that will make the process of building a url easy.\n",
    "def make_url(base_url , comp):\n",
    "    \n",
    "    url = base_url\n",
    "    \n",
    "    # add each component to the base url\n",
    "    for r in comp:\n",
    "        url = '{}/{}'.format(url, r)\n",
    "        \n",
    "    return url\n",
    "\n",
    "# function to get unique values \n",
    "def unique(list1): \n",
    "    x = np.array(list1) \n",
    "    return(np.unique(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-b63f5b428ec0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m#                 print(document)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                     \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m                     \u001b[0mdoc_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                     \u001b[0mdoc_content_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_content\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'directory'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'item'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#define our base url \n",
    "base_url = r\"https://www.sec.gov/Archives/edgar/data\"\n",
    "cik_num = '/1318605/'\n",
    "\n",
    "\n",
    "#lets create a filing url \n",
    "filings_url = base_url +cik_num + '/index.json'\n",
    "content = requests.get(filings_url)\n",
    "decoded_content = content.json()\n",
    "\n",
    "filings = decoded_content['directory']['item']\n",
    "\n",
    "summary_filing_list = []\n",
    "\n",
    "\n",
    "\n",
    "#iterate through each filing for GS\n",
    "for filing in filings: \n",
    "    \n",
    "    #define each filing number \n",
    "    filing_num = filing['name']\n",
    "    \n",
    "    filing_url = base_url + cik_num + filing_num + '/index.json'\n",
    "    print(\"Filing URL\")\n",
    "    print(filing_url)\n",
    "    \n",
    "    try: \n",
    "        content = requests.get(filing_url)\n",
    "        sleep(0.2)\n",
    "        document_content = content.json()\n",
    "        \n",
    "        doc_directory = pd.DataFrame(document_content['directory']['item'])\n",
    "        \n",
    "        deduped_doc_directory = doc_directory.drop_duplicates(subset = ['name'])\n",
    "        #print(deduped_doc_directory )\n",
    "        \n",
    "        #iterate through each document in each filing \n",
    "        for index,document in deduped_doc_directory.iterrows():\n",
    "            #print(document['name'])\n",
    "            if document['type'] != 'image2.gif' and '.txt' in document['name']:\n",
    "                doc_name = document['name']\n",
    "                document_url = base_url + cik_num + filing_num + '/index.json'\n",
    "#                 print(document)\n",
    "                try: \n",
    "                    sleep(0.2)\n",
    "                    doc_content = requests.get(document_url).json()\n",
    "                    doc_content_df = pd.DataFrame(doc_content['directory']['item'])\n",
    "                    #print(doc_content_df)\n",
    "                    \n",
    "                    deduped_doc_content = doc_content_df.drop_duplicates(subset = ['name'])\n",
    "                    #print(deduped_doc_content)\n",
    "                    #doc_content = content.json()\n",
    "                    #print(doc_content)\n",
    "                    #print(document_url)\n",
    "                    \n",
    "                    for index, file in deduped_doc_content.iterrows():\n",
    "                        if file['name'] == 'FilingSummary.xml':\n",
    "                            #print(file['name'])\n",
    "                            xml_summary = \"https://www.sec.gov\" + doc_content['directory']['name'] +'/' + file['name']\n",
    "                            #print(xml_summary)\n",
    "                            summary_filing_list.append(xml_summary)\n",
    "                except Exception:\n",
    "                    print(\"failed to get filing summaries\")\n",
    "                    pass\n",
    "                    \n",
    "    except Exception: \n",
    "        print(\"failed to get json:\" + filing_url)\n",
    "\n",
    "#dedupe unique filing summaries and store in dataframe \n",
    "summary_filing_df = pd.DataFrame(summary_filing_list, columns = ['xml_summary'])\n",
    "summary_filing_df = summary_filing_df.drop_duplicates() \n",
    "print(summary_filing_df)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "print(\"finished compiling summary urls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ingest summary df and pull reports, write to csv \n",
    "def pull_reports(df):\n",
    "        #iterate through df and get financial data reports in a master list of dictionaries\n",
    "    print(\"Starting to pull reports\")\n",
    "    start = time.time()\n",
    "\n",
    "    master_reports = []\n",
    "\n",
    "    stmt_list = []\n",
    "\n",
    "    category_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        try: \n",
    "            xml = row['xml_summary']\n",
    "            base_url = row['xml_summary'].replace('FilingSummary.xml', '')\n",
    "            #print(xml)\n",
    "            content = requests.get(xml).content\n",
    "            sleep(0.25)\n",
    "            soup = BeautifulSoup(content, 'lxml')\n",
    "            # find the 'myreports' tag because this contains all the individual reports submitted.\n",
    "            reports = soup.find('myreports') \n",
    "            # print(reports) \n",
    "            # loop through each report in the 'myreports' tag but avoid the last one as this will cause an error.\n",
    "            for report in reports.find_all('report')[:-1]:\n",
    "\n",
    "                # let's create a dictionary to store all the different parts we need.\n",
    "                report_dict = {}\n",
    "                report_dict['name_short'] = report.shortname.text.lower()\n",
    "                report_dict['name_long'] = report.longname.text\n",
    "\n",
    "                # position and category isnt always available\n",
    "                try: \n",
    "                    report_dict['position'] = report.position.text\n",
    "                    report_dict['category'] = report.menucategory.text\n",
    "                except Exception as ex:\n",
    "                    report_dict['position'] = ''\n",
    "                    report_dict['category'] = ''\n",
    "\n",
    "                report_dict['url'] = base_url + report.htmlfilename.text\n",
    "\n",
    "                # only add statements accounting for missing categories\n",
    "                if (report_dict['category'] == \"Statements\" or report_dict['category'] =='') and \"consolidated\" in report_dict['name_short'] and \"unaudited\" not in report_dict['name_short']: \n",
    "\n",
    "                    # append the dictionary to the master list.\n",
    "                    master_reports.append(report_dict)\n",
    "\n",
    "#                     stmt_list.append(report_dict['name_short'])\n",
    "#                     category_list.append(report_dict['category'])\n",
    "                    #print(report_dict)\n",
    "                else: \n",
    "                    pass\n",
    "        except Exception as ex:\n",
    "            print(\"failed\")\n",
    "\n",
    "    #         # print the info to the user.\n",
    "    #         print('-'*100)\n",
    "    #         print(base_url + report.htmlfilename.text)\n",
    "    #         print(report.longname.text)\n",
    "    #         print(report.shortname.text)\n",
    "    #         print(report.menucategory.text)\n",
    "    #         print(report.position.text)\n",
    "\n",
    "    #End Code\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    print(\"Finished Pulling Reports\")\n",
    "    \n",
    "    print(\"Writing reports to csv\")\n",
    "    #write reports to csv\n",
    "    pd.DataFrame(master_reports).to_csv(\"example_master_reports.csv\")\n",
    "    \n",
    "    #pull from csv\n",
    "    master_reports_extract = pd.read_csv(\"example_master_reports.csv\")\n",
    "    print(master_reports_extract)\n",
    "    return(master_reports_extract)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to pull reports\n",
      "20.694371700286865\n",
      "Finished Pulling Reports\n",
      "Writing reports to csv\n",
      "     Unnamed: 0                                         name_short  \\\n",
      "0             0                        consolidated balance sheets   \n",
      "1             1        consolidated balance sheets (parenthetical)   \n",
      "2             2              consolidated statements of operations   \n",
      "3             3      consolidated statements of comprehensive loss   \n",
      "4             4  consolidated statements of redeemable noncontr...   \n",
      "..          ...                                                ...   \n",
      "125         125    condensed consolidated statements of cash flows   \n",
      "126         126              condensed consolidated balance sheets   \n",
      "127         127  condensed consolidated balance sheets (parenth...   \n",
      "128         128    condensed consolidated statements of operations   \n",
      "129         129    condensed consolidated statements of cash flows   \n",
      "\n",
      "                                             name_long  position    category  \\\n",
      "0     100010 - Statement - Consolidated Balance Sheets       2.0  Statements   \n",
      "1    100020 - Statement - Consolidated Balance Shee...       3.0  Statements   \n",
      "2    100030 - Statement - Consolidated Statements o...       4.0  Statements   \n",
      "3    100040 - Statement - Consolidated Statements o...       5.0  Statements   \n",
      "4    100050 - Statement - Consolidated Statements o...       6.0  Statements   \n",
      "..                                                 ...       ...         ...   \n",
      "125  00300 - Statement - Condensed Consolidated Sta...       NaN         NaN   \n",
      "126  00100 - Statement - Condensed Consolidated Bal...       NaN         NaN   \n",
      "127  00105 - Statement - Condensed Consolidated Bal...       NaN         NaN   \n",
      "128  00200 - Statement - Condensed Consolidated Sta...       NaN         NaN   \n",
      "129  00300 - Statement - Condensed Consolidated Sta...       NaN         NaN   \n",
      "\n",
      "                                                   url  \n",
      "0    https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "1    https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "2    https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "3    https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "4    https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "..                                                 ...  \n",
      "125  https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "126  https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "127  https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "128  https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "129  https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "\n",
      "[130 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "report_df = pull_reports(summary_filing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(report_df).to_csv(\"example_master_reports.csv\")\n",
    "\n",
    "#pull from csv\n",
    "report_extract = pd.read_csv(\"example_master_reports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0  Unnamed: 0.1  \\\n",
      "0             0             0   \n",
      "1             1             1   \n",
      "2             2             2   \n",
      "3             3             3   \n",
      "4             4             4   \n",
      "..          ...           ...   \n",
      "125         125           125   \n",
      "126         126           126   \n",
      "127         127           127   \n",
      "128         128           128   \n",
      "129         129           129   \n",
      "\n",
      "                                            name_short  \\\n",
      "0                          consolidated balance sheets   \n",
      "1          consolidated balance sheets (parenthetical)   \n",
      "2                consolidated statements of operations   \n",
      "3        consolidated statements of comprehensive loss   \n",
      "4    consolidated statements of redeemable noncontr...   \n",
      "..                                                 ...   \n",
      "125    condensed consolidated statements of cash flows   \n",
      "126              condensed consolidated balance sheets   \n",
      "127  condensed consolidated balance sheets (parenth...   \n",
      "128    condensed consolidated statements of operations   \n",
      "129    condensed consolidated statements of cash flows   \n",
      "\n",
      "                                             name_long  position    category  \\\n",
      "0     100010 - Statement - Consolidated Balance Sheets       2.0  Statements   \n",
      "1    100020 - Statement - Consolidated Balance Shee...       3.0  Statements   \n",
      "2    100030 - Statement - Consolidated Statements o...       4.0  Statements   \n",
      "3    100040 - Statement - Consolidated Statements o...       5.0  Statements   \n",
      "4    100050 - Statement - Consolidated Statements o...       6.0  Statements   \n",
      "..                                                 ...       ...         ...   \n",
      "125  00300 - Statement - Condensed Consolidated Sta...       NaN         NaN   \n",
      "126  00100 - Statement - Condensed Consolidated Bal...       NaN         NaN   \n",
      "127  00105 - Statement - Condensed Consolidated Bal...       NaN         NaN   \n",
      "128  00200 - Statement - Condensed Consolidated Sta...       NaN         NaN   \n",
      "129  00300 - Statement - Condensed Consolidated Sta...       NaN         NaN   \n",
      "\n",
      "                                                   url  \n",
      "0    https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "1    https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "2    https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "3    https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "4    https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "..                                                 ...  \n",
      "125  https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "126  https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "127  https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "128  https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "129  https://www.sec.gov/Archives/edgar/data/131860...  \n",
      "\n",
      "[130 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(report_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reports(df):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
