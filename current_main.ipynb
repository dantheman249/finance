{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniel\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pandas_datareader\\compat\\__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from current_scraper.ipynb\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import datetime as dt \n",
    "import lxml\n",
    "from lxml import html, etree\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader\n",
    "from pandas_datareader import data as pdr\n",
    "import time\n",
    "from time import sleep\n",
    "import string\n",
    "import yfinance as yf\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import io\n",
    "import os\n",
    "import pandasql\n",
    "from pandasql import sqldf\n",
    "import re\n",
    "import gc\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import psutil\n",
    "\n",
    "from collections import Counter\n",
    "import linecache\n",
    "import tracemalloc\n",
    "\n",
    "import reshape\n",
    "\n",
    "import import_ipynb\n",
    "import current_scraper\n",
    "from current_scraper import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\GitHub\\\\finance'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top(snapshot, key_type='lineno', limit=3):\n",
    "    snapshot = snapshot.filter_traces((\n",
    "        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "        tracemalloc.Filter(False, \"<unknown>\"),\n",
    "    ))\n",
    "    top_stats = snapshot.statistics(key_type)\n",
    "\n",
    "    print(\"Top %s lines\" % limit)\n",
    "    for index, stat in enumerate(top_stats[:limit], 1):\n",
    "        frame = stat.traceback[0]\n",
    "        # replace \"/path/to/module/file.py\" with \"module/file.py\"\n",
    "        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\n",
    "        print(\"#%s: %s:%s: %.1f KiB\"\n",
    "              % (index, filename, frame.lineno, stat.size / 1024))\n",
    "        line = linecache.getline(frame.filename, frame.lineno).strip()\n",
    "        if line:\n",
    "            print('    %s' % line)\n",
    "\n",
    "    other = top_stats[limit:]\n",
    "    if other:\n",
    "        size = sum(stat.size for stat in other)\n",
    "        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\n",
    "    total = sum(stat.size for stat in top_stats)\n",
    "    print(\"Total allocated size: %.1f KiB\" % (total / 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ticker      cik\n",
      "0      aal     6201\n",
      "1      aap  1158449\n",
      "2     aapl   320193\n",
      "3     abbv  1551152\n",
      "4      abc  1140859\n",
      "..     ...      ...\n",
      "495    yum  1041061\n",
      "496    zbh  1136869\n",
      "497   zbra   877212\n",
      "498   zion   109380\n",
      "499    zts  1555280\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#get ticker, company name and CIK numbers \n",
    "ticker_url = \"https://www.sec.gov/include/ticker.txt\"\n",
    "ticker_request = requests.get(ticker_url).content\n",
    "ticker_df = pd.read_csv(io.StringIO(ticker_request.decode('utf-8')),sep=\"\\t\")\n",
    "ticker_df.columns = ['ticker', 'cik']\n",
    "ticker_df[\"ticker\"] = ticker_df[\"ticker\"].str.lower()\n",
    "\n",
    "\n",
    "#get sp500 tickers \n",
    "sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "sp500 = pd.DataFrame(sp500[0]['Symbol'])\n",
    "sp500.columns = ['ticker']\n",
    "sp500[\"ticker\"] = sp500[\"ticker\"].str.lower()\n",
    "#print(sp500)\n",
    "\n",
    "#get cik of sp 500\n",
    "sample = pd.merge(ticker_df,sp500,on='ticker')\n",
    "print(sample )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get tickers in the sp 500 and get easy stats on them\n",
    "#get avg price by year, and current price of stock \n",
    "def get_easy_stats(df):\n",
    "    \n",
    "    easy_df = []\n",
    "    \n",
    "    start_dt = dt.datetime(2013,1,1)\n",
    "    end_dt = dt.datetime(2019,12,31)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        try: \n",
    "        \n",
    "            sleep(0.2)\n",
    "            #data_ts = pdr.get_data_yahoo(row['ticker'], start_dt, end_dt)\n",
    "            data_ts = yf.download(row['ticker'], start= start_dt, end= end_dt, progress=False)\n",
    "            #print(data_ts)\n",
    "                                      \n",
    "            data_ts['year'] = data_ts.index.year\n",
    "            #get average price per share \n",
    "            agg_df = data_ts.groupby('year', as_index=False)['Adj Close'].mean()\n",
    "            agg_df['ticker'] = row['ticker']\n",
    "            agg_df.columns = ['year','avg_price', 'ticker']\n",
    "            agg_df['year']=  agg_df['year'].astype(str)\n",
    "    #       print(agg_df)\n",
    "            current_price = pdr.get_data_yahoo(row['ticker']).last('1D')['Adj Close'][0].astype(int)\n",
    "\n",
    "            #turn agg_df wide  for price rows\n",
    "            wide_df = agg_df.pivot(index='ticker', columns='year', values=['avg_price'])\n",
    "\n",
    "            wide_df.columns = list(map(\"\".join, wide_df.columns))\n",
    "            wide_df['current_price'] = current_price\n",
    "            print(wide_df)\n",
    "\n",
    "            easy_df.append(wide_df)\n",
    "            del agg_df\n",
    "            del wide_df\n",
    "        \n",
    "        except Exception as ex:\n",
    "            print(row['ticker'] + \" failed\")\n",
    "            print(ex)\n",
    "            \n",
    "    \n",
    "    easy_df = pd.concat(easy_df)\n",
    "    #easy_df.to_csv(\"easy_stats_sp500.csv\")\n",
    "    return(easy_df)\n",
    "#function end\n",
    "\n",
    "start = time.time()\n",
    "#run to generate easy stat csv\n",
    "easy_stat_df = get_easy_stats(sample)\n",
    "easy_stat_df.to_csv(\"easy_stats_sp500.csv\")\n",
    "print(easy_stat_df)\n",
    "\n",
    "end = time.time()\n",
    "print(\"finished getting easy stats for sp500\")\n",
    "print(end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the easy stats of sp500\n",
    "#easy_stat_df.to_csv(\"easy_stats_sp500.csv\")\n",
    "easy_stat_df = pd.read_csv(\"easy_stats_sp500.csv\")\n",
    "print(easy_stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up script to traverse directory of quarterly submissions\n",
    "def generate_finance_table_csv():\n",
    "\n",
    "    #store all finances in a table\n",
    "    finance_list = []\n",
    "\n",
    "    #go into directory with data\n",
    "    file_os = 'D:\\\\Finance Data'\n",
    "    \n",
    "    \n",
    "    code_os = 'D:\\\\GitHub\\\\finance'\n",
    "    start = time.time()\n",
    "    \n",
    "    #go into file directory\n",
    "    os.chdir(file_os)\n",
    "    \n",
    "    for i in os.listdir():\n",
    "        file_path = i\n",
    "        #if its a quarterly filing data, parse it\n",
    "        if 'q1' in file_path or 'q2' in file_path or 'q3' in file_path or 'q4' in file_path:\n",
    "            try:\n",
    "                tracemalloc.start()\n",
    "                print(\"starting: \" + file_path)\n",
    "\n",
    "                sub = pd.read_csv(file_path + '/' + 'sub.txt', sep = '\\t', encoding = \"ISO-8859-1\", iterator = True, chunksize =100000)\n",
    "                sub = pd.concat(sub)\n",
    "                #clean up form column and filter to just 10-K submissions\n",
    "                sub['form'] = sub['form'].replace(np.nan, '', regex=True)\n",
    "                sub = sub[sub['form'] == '10-K']\n",
    "                #filter to the s&p500\n",
    "                sub = pd.merge(sub,sample, on = ['cik'])\n",
    "\n",
    "                pre = pd.read_csv(file_path + '/' + 'pre.txt', sep = '\\t', encoding = \"ISO-8859-1\", chunksize =1000000) \n",
    "                num = pd.read_csv(file_path + '/' + 'num.txt', sep = '\\t', encoding = \"ISO-8859-1\",  chunksize =100000)\n",
    "\n",
    "                print(\"joining num to sub\")\n",
    "                num_sub = pd.DataFrame()              \n",
    "                #join in numbers to submissions in chunks\n",
    "                for chunks in num: \n",
    "                    #print(chunks)\n",
    "                    num_sub = pd.concat([num_sub, sub.merge(chunks, on=['adsh'])])\n",
    "\n",
    "                #no longer need sub\n",
    "                del sub\n",
    "\n",
    "                print(\"joining pre to numsub\")\n",
    "                nsp = pd.DataFrame()\n",
    "                #add in plabel and stmt info\n",
    "                for chunks in pre:\n",
    "                    #print(chunks)\n",
    "                    nsp = pd.concat([nsp, num_sub.merge(chunks, on=['adsh', 'tag', 'version'])])\n",
    "                    nsp = nsp[nsp['stmt'].isin(['BS', 'IS', 'CF', 'CI', 'EQ'])]\n",
    "                # delete files no longer needed\n",
    "                del num_sub\n",
    "                del pre\n",
    "                del num\n",
    "\n",
    "\n",
    "                #print(nsp.columns)\n",
    "\n",
    "                nsp = nsp[['name', 'sic', 'fye', 'form', 'period', \n",
    "                           'fy', 'fp', 'filed','ticker', 'cik','ddate', \n",
    "                           'qtrs', 'uom', 'value', 'adsh','stmt',  'tag', \n",
    "                           'version', 'plabel']]\n",
    "\n",
    "                #add finances to master table\n",
    "                finance_list.append(nsp)\n",
    "                del nsp\n",
    "                print(\"processed_data: \" + file_path)\n",
    "                gc.collect()\n",
    "                snapshot = tracemalloc.take_snapshot()\n",
    "                display_top(snapshot)\n",
    "            except Exception as ex:\n",
    "                print(\"failed: \" + file_path)\n",
    "                print(str(ex))\n",
    "    \n",
    "    #switch back to github directory \n",
    "    os.chdir(code_os)\n",
    "    \n",
    "    \n",
    "    finance_table = pd.concat(finance_list)\n",
    "    del finance_list \n",
    "    finance_table.to_csv(\"total_finance_table.csv\")\n",
    "    end = time.time()\n",
    "    print(\"finished traversing files in X secs\")\n",
    "    print(end - start)\n",
    "    #return(finance_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run function to generate finance table csv\n",
    "generate_finance_table_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function goes through and tries to parse out the historical data we need: VERY LONG \n",
    "def parse_data(df):\n",
    "    gc.collect()\n",
    "    ############################################################\n",
    "    #process data a bit \n",
    "    \n",
    "    #turn things into an integer\n",
    "    df['qtrs'] = df.qtrs.astype(int)\n",
    "    df['period'] = df.period.astype(int)\n",
    "    df['fy'] = df.fy.astype(int)\n",
    "    df['fye'] = df.fye.astype(int)\n",
    "    df['ddate'] = df.ddate.astype(int)\n",
    "    df['ddate_prev'] = df.ddate.astype(int) + 10000\n",
    "    \n",
    "    #turn things to strings\n",
    "    df['period'] = df['period'].astype(str)\n",
    "    df['fye'] = df['fye'].astype(str)\n",
    "    df['fy'] = df['fy'].astype(str)\n",
    "    df['ddate'] = df['ddate'].astype(str)\n",
    "    df['ddate_prev'] = df['ddate_prev'].astype(str)\n",
    "    #pad fye with a leading 0 \n",
    "    df['fye'] = df['fye'].apply(lambda x: x.zfill(4))\n",
    "    \n",
    "    #fill nas inplace = true changes the df directly\n",
    "    df['period'].fillna('', inplace=True)\n",
    "    df['fy'].fillna('', inplace=True)\n",
    "    df['fye'].fillna('', inplace=True)\n",
    "    df['plabel'].fillna('', inplace=True)\n",
    "    df['ddate'].fillna('', inplace=True)\n",
    "    df['ddate_prev'].fillna('', inplace=True)\n",
    "    \n",
    "    #create a joinable key\n",
    "    df['id'] = df['fy'].astype(str) + '_' + df['cik'].astype(str)\n",
    "\n",
    "    #get distinct company and years\n",
    "    key_df = pd.DataFrame(df.id.unique(), columns = ['id'])\n",
    "    gc.collect()\n",
    "    \n",
    "    #######################################################################################################\n",
    "    #define tags\n",
    "    \n",
    "    #for shares outstanding calc (all qtrs = 4, income stmt)\n",
    "    shares_outstanding_tags = ['WeightedAverageNumberOfDilutedSharesOutstanding']\n",
    "    net_income_tags = ['NetIncomeLoss']\n",
    "    eps_tags = ['EarningsPerShareDiluted']\n",
    "    \n",
    "    # for fcfe calculation  (cash flow stmt)\n",
    "    cfo_tags = ['NetCashProvidedByUsedInOperatingActivities','NetCashProvidedByUsedInOperatingActivitiesContinuingOperations']\n",
    "    cfo_plabels = ['Cash generated by operating activities']\n",
    "    debt_repayments_tags= ['RepaymentsOfDebt', 'RepaymentsOfLongTermDebt', 'RepaymentsOfOtherLongTermDebt',\n",
    "                           'RepaymentsOfDebtAndCapitalLeaseObligations','RepaymentsOfLongTermDebtAndCapitalLeaseObligations',\n",
    "                          'RepaymentsOfLongTermDebtAndCapitalSecurities', 'RepaymentsOfLongTermCapitalLeaseObligations']\n",
    "    debt_repayments_plabels = ['Repayments of long-term debt and finance lease obligations', \n",
    "                               'Maturities of long-term debt', 'Repayments of recourse debt']\n",
    "    debt_proceeds_tags = ['ProceedsFromIssuanceOfDebt', 'ProceedsFromIssuanceOfLongTermDebt']\n",
    "    \n",
    "    #for capital expenditure calculations (cash flow statement)\n",
    "    capex_tags = ['PaymentsToAcquireProductiveAssets']\n",
    "    capex_plabels = ['Capital expenditures']\n",
    "    ppe_purchase_tags = ['PaymentsToAcquirePropertyPlantAndEquipment', 'PaymentsToAcquireProductiveAssets']\n",
    "    ppe_sale_tags = ['ProceedsFromSaleOfPropertyPlantAndEquipment', \n",
    "                     'ProceedsFromSaleOfPropertyPlantAndEquipmentAndSaleLeaseBackTransactions']\n",
    "    #qtrs = 0; for ppe on balance sheet\n",
    "    ppe_tags = ['PropertyPlantAndEquipmentNet']\n",
    "\n",
    "    #for ebitda calculation\n",
    "    revenue_tags = ['Revenues', 'RevenueFromContractWithCustomerExcludingAssessedTax', 'SalesRevenueNet','SalesRevenueServicesNet']\n",
    "    revenue_plabels = ['Net Sales']\n",
    "    \n",
    "    operating_income_tags = ['OperatingIncomeLoss']\n",
    "    dep_amort_tags = ['DepreciationDepletionAndAmortization', 'Depreciation']\n",
    "    dep_amort_plabels = ['Depreciation and amortization']\n",
    "    \n",
    "    #for enterprise value calculation\n",
    "    ltd_tags = ['LongTermDebtNoncurrent', 'LongTermDebtAndCapitalLeaseObligations']\n",
    "    std_tags = ['LongTermDebtCurrent', 'LongTermDebtAndCapitalLeaseObligationsCurrent']\n",
    "    cash_tags = ['CashAndCashEquivalentsAtCarryingValue', 'Cash']\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    #######################################################################################################\n",
    "    #pull rows\n",
    "    \n",
    "#shares outstanding\n",
    "    shares_outstanding_rows = df[\n",
    "        (df['tag'].isin(shares_outstanding_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.uom.str.contains('shares',na=False))\n",
    "    ]\n",
    "\n",
    "    shares_outstanding_agg = shares_outstanding_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    shares_outstanding_agg.columns = ['id', 'shares_outstanding']\n",
    "    del shares_outstanding_rows\n",
    "#net income\n",
    "    net_income_rows = df[\n",
    "        (df['tag'].isin(net_income_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    net_income_agg = net_income_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    net_income_agg.columns = ['id', 'net_income']\n",
    "    del net_income_rows\n",
    "#eps\n",
    "    eps_rows = df[\n",
    "        (df['tag'].isin(eps_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    eps_agg = eps_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    eps_agg.columns = ['id', 'eps']\n",
    "    del eps_rows\n",
    "#cash flow from operations    \n",
    "    cfo_rows = df[\n",
    "        (df['tag'].isin(cfo_tags) | df['plabel'].isin(cfo_plabels))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    cfo_agg = cfo_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    cfo_agg.columns = ['id', 'cfo']\n",
    "    del cfo_rows\n",
    "#debt repayment \n",
    "    debt_repayments_rows = df[\n",
    "        (df['tag'].isin(debt_repayments_tags)| df['plabel'].isin(debt_repayments_plabels))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    debt_repayments_agg = debt_repayments_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    debt_repayments_agg.columns = ['id', 'debt_repayments']\n",
    "    del debt_repayments_rows\n",
    "#debt proceeds \n",
    "    debt_proceeds_rows = df[\n",
    "        (df['tag'].isin(debt_proceeds_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    debt_proceeds_agg = debt_proceeds_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    debt_proceeds_agg.columns = ['id', 'debt_proceeds']\n",
    "    del debt_proceeds_rows\n",
    "#cap ex\n",
    "    capex_rows = df[\n",
    "        (df['tag'].isin(capex_tags)| df['plabel'].isin(capex_plabels))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    capex_agg = capex_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    capex_agg.columns = ['id', 'capex']\n",
    "    del capex_rows\n",
    "#ppe purchase\n",
    "    ppe_purchase_rows = df[\n",
    "        (df['tag'].isin(ppe_purchase_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    ppe_purchase_agg = ppe_purchase_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    ppe_purchase_agg.columns = ['id', 'ppe_purchase']\n",
    "    del ppe_purchase_rows\n",
    "#ppe sale \n",
    "    ppe_sale_rows = df[\n",
    "        (df['tag'].isin(ppe_sale_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    ppe_sale_agg = ppe_sale_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    ppe_sale_agg.columns = ['id', 'ppe_sale']\n",
    "    del ppe_sale_rows\n",
    "#ppe , qtrs = 0; for ppe on balance sheet\n",
    "    ppe_rows = df[\n",
    "        (df['tag'].isin(ppe_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 0)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    ppe_agg = ppe_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    ppe_agg.columns = ['id', 'ppe']\n",
    "    del ppe_rows\n",
    "#get PPE of the year before\n",
    "    ppe_prev_rows = df[\n",
    "        (df['tag'].isin(ppe_tags))\n",
    "        & (df['ddate_prev'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 0)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    ppe_prev_agg = ppe_prev_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    ppe_prev_agg.columns = ['id', 'ppe_prev']\n",
    "    del ppe_prev_rows\n",
    "#revenue \n",
    "    revenue_rows = df[\n",
    "        (df['tag'].isin(revenue_tags) | df['plabel'].isin(revenue_plabels))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    revenue_agg = revenue_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    revenue_agg.columns = ['id', 'revenue']\n",
    "    del revenue_rows\n",
    "#operating income\n",
    "    operating_income_rows = df[\n",
    "        (df['tag'].isin(operating_income_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    operating_income_agg = operating_income_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    operating_income_agg.columns = ['id', 'operating_income']\n",
    "    del operating_income_rows\n",
    "#depreciation and amortization\n",
    "    dep_amort_rows = df[\n",
    "        (df['tag'].isin(dep_amort_tags) | df['plabel'].isin(dep_amort_plabels))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    dep_amort_agg = dep_amort_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    dep_amort_agg.columns = ['id', 'dep_amort']\n",
    "    del dep_amort_rows\n",
    "\n",
    "#     #get long term debt on balance sheet\n",
    "    ltd_rows = df[\n",
    "        (df['tag'].isin(ltd_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    ltd_agg = ltd_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    ltd_agg.columns = ['id', 'ltd']\n",
    "    del ltd_rows\n",
    "#     #get short term debt on balance sheet \n",
    "    std_rows = df[\n",
    "        (df['tag'].isin(std_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    std_agg = std_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    std_agg.columns = ['id', 'std']\n",
    "    del std_rows\n",
    "#     #get cash and cash equivalents\n",
    "    cash_rows = df[\n",
    "        (df['tag'].isin(cash_tags))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    cash_agg = cash_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    cash_agg.columns = ['id', 'cash']\n",
    "    del cash_rows\n",
    "    \n",
    "#     #######################################################################################################\n",
    "    #add financials to key_df\n",
    "    merged_df = key_df.merge(shares_outstanding_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(net_income_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(eps_agg, on = ['id'], how = \"left\")\n",
    "    \n",
    "    merged_df = merged_df.merge(cfo_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(debt_proceeds_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(debt_repayments_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(capex_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(ppe_purchase_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(ppe_sale_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(ppe_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(ppe_prev_agg, on = ['id'], how = \"left\")\n",
    "    \n",
    "    merged_df = merged_df.merge(revenue_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(operating_income_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(dep_amort_agg, on = ['id'], how = \"left\")\n",
    "    \n",
    "    merged_df = merged_df.merge(ltd_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(std_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(cash_agg, on = ['id'], how = \"left\")\n",
    "    \n",
    "    \n",
    "#     #######################################################################################################\n",
    "#     #delete unneeded things \n",
    "    del shares_outstanding_agg\n",
    "    del net_income_agg\n",
    "    del eps_agg\n",
    "    del cfo_agg\n",
    "    del debt_proceeds_agg\n",
    "    del debt_repayments_agg\n",
    "    del capex_agg\n",
    "    del ppe_purchase_agg\n",
    "    del ppe_sale_agg\n",
    "    del ppe_agg\n",
    "    del ppe_prev_agg\n",
    "    del revenue_agg\n",
    "    del operating_income_agg\n",
    "    del dep_amort_agg\n",
    "    return(merged_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_value(x):\n",
    "    if x.first_valid_index() is None:\n",
    "        return None\n",
    "    else:\n",
    "        return x[x.first_valid_index()]\n",
    "    \n",
    "def last_value(x):\n",
    "    if x.last_valid_index() is None:\n",
    "        return None\n",
    "    else:\n",
    "        return x[x.last_valid_index()]\n",
    "    \n",
    "#function that runs yahoo querying on sample tickers\n",
    "def get_yahoo_info():\n",
    "    yahoo_info = get_info(sample['ticker'])\n",
    "    yahoo_info.to_csv(\"yahoo_info.csv\")\n",
    "    return(yahoo_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the data in chunks first, return ticker level dataset \n",
    "def init_process_data():\n",
    "    \n",
    "    processed_list = []\n",
    "    \n",
    "    chunksize = 10 ** 5\n",
    "    for chunk in pd.read_csv(\"total_finance_table.csv\", chunksize=chunksize):  \n",
    "        #for each chunk, get finance info on a company, yearly level\n",
    "        processed_list.append(parse_data(chunk))\n",
    "#         break\n",
    "    #turn list of dataframes into one dataframe\n",
    "    processed_table = pd.concat(processed_list)\n",
    "    \n",
    "\n",
    "    #split id back into year and cik\n",
    "    processed_table[['year','cik']] = processed_table.id.str.split(pat = \"_\", expand=True) \n",
    "    #print(\"printing pre-aggregated data\")\n",
    "    #print(processed_table)\n",
    "    del processed_list\n",
    "    #######################################################################################################\n",
    "    #aggregate up across chunks \n",
    "    agg_df = sqldf(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "    *, \n",
    "    fcfe/NULLIF(shares_outstanding_coalesced,0) AS fcfe_per_share\n",
    "    FROM \n",
    "    (\n",
    "        SELECT\n",
    "        *, \n",
    "        cfo - capex_coalesced +  IFNULL(debt_repayments,0) AS fcfe\n",
    "        FROM \n",
    "        (\n",
    "             SELECT \n",
    "            *,\n",
    "            IFNULL(operating_income,0) + IFNULL(dep_amort,0) AS ebitda, \n",
    "\n",
    "            CASE WHEN shares_outstanding IS NULL THEN net_income/NULLIF(eps,0) \n",
    "                 ELSE shares_outstanding \n",
    "                 END AS shares_outstanding_coalesced, \n",
    "\n",
    "            CASE WHEN capex IS NOT NULL THEN capex \n",
    "                 WHEN ppe_purchase IS NOT NULL THEN ppe_purchase - IFNULL(ppe_sale,0)\n",
    "                 WHEN ppe IS NOT NULL THEN ppe - IFNULL(ppe_prev,0) \n",
    "                 END AS capex_coalesced, \n",
    "\n",
    "            IFNULL(debt_proceeds,0) - IFNULL(debt_repayments,0) AS net_debt_proceeds, \n",
    "            \n",
    "            IFNULL(ltd,0) + IFNULL(std,0) AS total_debt \n",
    "            \n",
    "            FROM \n",
    "            (\n",
    "                SELECT  \n",
    "                year,\n",
    "                CAST(cik AS INT64) as cik,\n",
    "                MAX(shares_outstanding) AS shares_outstanding,\n",
    "                MAX(net_income) AS net_income,\n",
    "                MAX(eps) AS eps,\n",
    "                MAX(cfo) AS cfo,\n",
    "                MAX(debt_proceeds) AS debt_proceeds,\n",
    "                MAX(debt_repayments) AS debt_repayments,\n",
    "                MAX(capex) AS capex,\n",
    "                MAX(ppe_purchase) AS ppe_purchase,\n",
    "                MAX(ppe_sale) AS ppe_sale,\n",
    "                MAX(ppe) AS ppe,\n",
    "                MAX(ppe_prev) AS ppe_prev, \n",
    "                MAX(revenue) AS revenue,\n",
    "                MAX(operating_income) AS operating_income,\n",
    "                MAX(dep_amort) AS dep_amort, \n",
    "                MAX(ltd) AS ltd,\n",
    "                MAX(std) AS std, \n",
    "                MAX(cash) AS cash \n",
    "                FROM \n",
    "                processed_table \n",
    "                GROUP BY     \n",
    "                year,\n",
    "                cik\n",
    "                ORDER BY 2 DESC\n",
    "            )\n",
    "            )\n",
    "            \n",
    "    )\n",
    "    \n",
    "    \"\"\")\n",
    "    \n",
    "    del processed_table \n",
    "    #print(agg_df.columns)\n",
    "    \n",
    "#     #######################################################################################################\n",
    "    # turn long data wide \n",
    "    wide_df = agg_df.pivot(index='cik', columns='year', values=['shares_outstanding', 'net_income', 'eps', 'cfo',\n",
    "       'debt_proceeds', 'debt_repayments', 'capex', 'ppe_purchase', 'ppe_sale',\n",
    "       'ppe', 'ppe_prev', 'revenue', 'operating_income', 'dep_amort', 'ebitda',\n",
    "       'shares_outstanding_coalesced', 'capex_coalesced', 'net_debt_proceeds',\n",
    "       'fcfe', 'fcfe_per_share', 'ltd', 'std', 'cash'])\n",
    "    \n",
    "    del agg_df\n",
    "    \n",
    "#     ####################################################################################################### \n",
    "    #fix columns\n",
    "    wide_df.columns = list(map(\"\".join, wide_df.columns))\n",
    "    \n",
    "    #add in tickers\n",
    "    wide_df = wide_df.merge(sample, on = ['cik'])\n",
    "    \n",
    "    #drop weird columns \n",
    "    wide_df = wide_df[wide_df.columns.drop(list(wide_df.filter(regex='1218')))]\n",
    "    wide_df = wide_df[wide_df.columns.drop(list(wide_df.filter(regex='2020')))]\n",
    "    \n",
    "    #read in easy stats csv\n",
    "    easy_stat_df = pd.read_csv(\"easy_stats_sp500.csv\")\n",
    "    #read in yahoo info csv\n",
    "    yahoo_info = pd.read_csv(\"yahoo_info.csv\")\n",
    "    \n",
    "    \n",
    "    #join in easy stats\n",
    "    wide_df = wide_df.merge(easy_stat_df, on = 'ticker', how = 'left')\n",
    "    wide_df = wide_df.merge(yahoo_info, on = 'ticker', how = 'left')\n",
    "    \n",
    "    \n",
    "    ####################################################################################################### \n",
    "    # add dataquality columns - \n",
    "    #WTK: how many columns are available? first year, last year of data, years of data\n",
    "\n",
    "    revenue_cols =  [col for col in wide_df.columns if 'revenue' in col]\n",
    "    revenue_df = wide_df.loc[:,revenue_cols]\n",
    "    #make a copy to avoid setting warnings\n",
    "    copy_df = revenue_df.copy(deep=True) \n",
    "    revenue_df.loc[:,'revenue_last_value'] = copy_df.apply(last_value, axis=1)\n",
    "    revenue_df.loc[:,'revenue_last_year'] = copy_df.apply(lambda x: x.last_valid_index(), axis=1)\n",
    "    revenue_df.loc[:,'revenue_first_value'] = copy_df.apply(first_value, axis=1)\n",
    "    revenue_df.loc[:,'revenue_first_year'] = copy_df.apply(lambda x: x.first_valid_index(), axis=1)\n",
    "    revenue_df.loc[:,'revenue_years_avail'] = copy_df.apply(lambda x: x.count(), axis =1)\n",
    "    #massage year fields  \n",
    "    revenue_df[['revenue_first_year', 'revenue_last_year']] = revenue_df[['revenue_first_year', 'revenue_last_year']].fillna(value=0)\n",
    "    revenue_df[\"revenue_first_year\"] = revenue_df[\"revenue_first_year\"].str.replace(\"revenue\", \"\")\n",
    "    revenue_df[\"revenue_first_year\"].fillna(0, inplace = True)\n",
    "    revenue_df[\"revenue_first_year\"] = revenue_df[\"revenue_first_year\"].astype(int, errors = \"ignore\")\n",
    "    revenue_df[\"revenue_last_year\"] = revenue_df[\"revenue_last_year\"].str.replace(\"revenue\", \"\")\n",
    "    revenue_df[\"revenue_last_year\"].fillna(0, inplace = True)\n",
    "    revenue_df[\"revenue_last_year\"] = revenue_df[\"revenue_last_year\"].astype(int, errors = \"ignore\")\n",
    "    #get years between first and lasts date \n",
    "    #print(revenue_df.dtypes)\n",
    "    revenue_df['revenue_years_between'] = revenue_df['revenue_last_year'] - revenue_df['revenue_first_year']    \n",
    "    #add ticker back\n",
    "    revenue_df.loc[:,'ticker'] = wide_df['ticker']\n",
    "    revenue_df = revenue_df[[\n",
    "        'ticker',\n",
    "        'revenue_last_value',\n",
    "        'revenue_last_year',\n",
    "        'revenue_first_value',\n",
    "        'revenue_first_year',\n",
    "        'revenue_years_avail',\n",
    "        'revenue_years_between'\n",
    "    ]]\n",
    "    #print(revenue_df)\n",
    "    \n",
    "    #repeat above structure for shares and fcfe_modded\n",
    "    ####################################################################################################### \n",
    "    shares_outstanding_cols =  [col for col in wide_df.columns if 'shares_outstanding_coalesced' in col]\n",
    "    shares_outstanding_df = wide_df.loc[:,shares_outstanding_cols]\n",
    "    #make a copy to avoid setting warnings\n",
    "    copy_df = shares_outstanding_df.copy(deep=True) \n",
    "    shares_outstanding_df.loc[:,'shares_outstanding_last_value'] = copy_df.apply(last_value, axis=1)\n",
    "    shares_outstanding_df.loc[:,'shares_outstanding_last_year'] = copy_df.apply(lambda x: x.last_valid_index(), axis=1)\n",
    "    shares_outstanding_df.loc[:,'shares_outstanding_first_value'] = copy_df.apply(first_value, axis=1)\n",
    "    shares_outstanding_df.loc[:,'shares_outstanding_first_year'] = copy_df.apply(lambda x: x.first_valid_index(), axis=1)\n",
    "    shares_outstanding_df.loc[:,'shares_outstanding_years_avail'] = copy_df.apply(lambda x: x.count(), axis =1)\n",
    "    #massage year fields  \n",
    "    shares_outstanding_df[['shares_outstanding_first_year', 'shares_outstanding_last_year']] = shares_outstanding_df[['shares_outstanding_first_year', 'shares_outstanding_last_year']].fillna(value=0)\n",
    "    shares_outstanding_df[\"shares_outstanding_first_year\"] = shares_outstanding_df[\"shares_outstanding_first_year\"].str.replace(\"shares_outstanding_coalesced\", \"\")\n",
    "    shares_outstanding_df[\"shares_outstanding_first_year\"].fillna(0, inplace = True)\n",
    "    shares_outstanding_df[\"shares_outstanding_first_year\"] = shares_outstanding_df[\"shares_outstanding_first_year\"].astype(int, errors = \"ignore\")\n",
    "    shares_outstanding_df[\"shares_outstanding_last_year\"] = shares_outstanding_df[\"shares_outstanding_last_year\"].str.replace(\"shares_outstanding_coalesced\", \"\")\n",
    "    shares_outstanding_df[\"shares_outstanding_last_year\"].fillna(0, inplace = True)\n",
    "    shares_outstanding_df[\"shares_outstanding_last_year\"] = shares_outstanding_df[\"shares_outstanding_last_year\"].astype(int, errors = \"ignore\")\n",
    "    #print(shares_outstanding_df.dtypes)\n",
    "    #get years between first and lasts date \n",
    "    shares_outstanding_df['shares_outstanding_years_between'] = shares_outstanding_df['shares_outstanding_last_year'] - shares_outstanding_df['shares_outstanding_first_year']    \n",
    "    #add ticker back\n",
    "    shares_outstanding_df.loc[:,'ticker'] = wide_df['ticker'] \n",
    "    shares_outstanding_df = shares_outstanding_df[[\n",
    "        'ticker',\n",
    "        'shares_outstanding_last_value',\n",
    "        'shares_outstanding_last_year',\n",
    "        'shares_outstanding_first_value',\n",
    "        'shares_outstanding_first_year',\n",
    "        'shares_outstanding_years_avail',\n",
    "        'shares_outstanding_years_between'\n",
    "    ]]\n",
    "    #print(shares_outstanding_df)\n",
    "    \n",
    "    ####################################################################################################### \n",
    "    fcfe_cols =  [col for col in wide_df.columns if 'fcfe2' in col]\n",
    "    fcfe_df = wide_df.loc[:,fcfe_cols]\n",
    "    #make a copy to avoid setting warnings\n",
    "    copy_df = fcfe_df.copy(deep=True) \n",
    "    fcfe_df.loc[:,'fcfe_last_value'] = copy_df.apply(last_value, axis=1)\n",
    "    fcfe_df.loc[:,'fcfe_last_year'] = copy_df.apply(lambda x: x.last_valid_index(), axis=1)\n",
    "    fcfe_df.loc[:,'fcfe_first_value'] = copy_df.apply(first_value, axis=1)\n",
    "    fcfe_df.loc[:,'fcfe_first_year'] = copy_df.apply(lambda x: x.first_valid_index(), axis=1)\n",
    "    fcfe_df.loc[:,'fcfe_years_avail'] = copy_df.apply(lambda x: x.count(), axis =1)\n",
    "    #massage year fields  \n",
    "    fcfe_df[['fcfe_first_year', 'fcfe_last_year']] = fcfe_df[['fcfe_first_year', 'fcfe_last_year']].fillna(value=0)\n",
    "    fcfe_df[\"fcfe_first_year\"] = fcfe_df[\"fcfe_first_year\"].str.replace(\"fcfe\", \"\")\n",
    "    fcfe_df[\"fcfe_first_year\"].fillna(0, inplace = True)\n",
    "    fcfe_df[\"fcfe_first_year\"] = fcfe_df[\"fcfe_first_year\"].astype(int, errors = \"ignore\")\n",
    "    fcfe_df[\"fcfe_last_year\"] = fcfe_df[\"fcfe_last_year\"].str.replace(\"fcfe\", \"\")\n",
    "    fcfe_df[\"fcfe_last_year\"].fillna(0, inplace = True)\n",
    "    fcfe_df[\"fcfe_last_year\"] = fcfe_df[\"fcfe_last_year\"].astype(int, errors = \"ignore\")\n",
    "    #print(fcfe_df.dtypes)\n",
    "    #get years between first and lasts date \n",
    "    fcfe_df['fcfe_years_between'] = fcfe_df['fcfe_last_year'] - fcfe_df['fcfe_first_year']    \n",
    "    #add ticker back\n",
    "    fcfe_df.loc[:,'ticker'] = wide_df['ticker']   \n",
    "    fcfe_df = fcfe_df[[\n",
    "        'ticker',\n",
    "        'fcfe_last_value',\n",
    "        'fcfe_last_year',\n",
    "        'fcfe_first_value',\n",
    "        'fcfe_first_year',\n",
    "        'fcfe_years_avail',\n",
    "        'fcfe_years_between'\n",
    "    ]]\n",
    "    #print(fcfe_df)\n",
    "    \n",
    "    ####################################################################################################### \n",
    "    fcfe_per_share_cols =  [col for col in wide_df.columns if 'fcfe_per_share' in col]\n",
    "    fcfe_per_share_df = wide_df.loc[:,fcfe_per_share_cols]\n",
    "    #make a copy to avoid setting warnings\n",
    "    copy_df = fcfe_per_share_df.copy(deep=True) \n",
    "    fcfe_per_share_df.loc[:,'fcfe_per_share_last_value'] = copy_df.apply(last_value, axis=1)\n",
    "    fcfe_per_share_df.loc[:,'fcfe_per_share_last_year'] = copy_df.apply(lambda x: x.last_valid_index(), axis=1)\n",
    "    fcfe_per_share_df.loc[:,'fcfe_per_share_first_value'] = copy_df.apply(first_value, axis=1)\n",
    "    fcfe_per_share_df.loc[:,'fcfe_per_share_first_year'] = copy_df.apply(lambda x: x.first_valid_index(), axis=1)\n",
    "    fcfe_per_share_df.loc[:,'fcfe_per_share_years_avail'] = copy_df.apply(lambda x: x.count(), axis =1)\n",
    "    #massage year fields  \n",
    "    fcfe_per_share_df[['fcfe_per_share_first_year', 'fcfe_per_share_last_year']] = fcfe_per_share_df[['fcfe_per_share_first_year', 'fcfe_per_share_last_year']].fillna(value=0)\n",
    "    fcfe_per_share_df[\"fcfe_per_share_first_year\"] = fcfe_per_share_df[\"fcfe_per_share_first_year\"].str.replace(\"fcfe_per_share\", \"\")\n",
    "    fcfe_per_share_df[\"fcfe_per_share_first_year\"].fillna(0, inplace = True)\n",
    "    fcfe_per_share_df[\"fcfe_per_share_first_year\"] = fcfe_per_share_df[\"fcfe_per_share_first_year\"].astype(int, errors = \"ignore\")\n",
    "    fcfe_per_share_df[\"fcfe_per_share_last_year\"] = fcfe_per_share_df[\"fcfe_per_share_last_year\"].str.replace(\"fcfe_per_share\", \"\")\n",
    "    fcfe_per_share_df[\"fcfe_per_share_last_year\"].fillna(0, inplace = True)\n",
    "    fcfe_per_share_df[\"fcfe_per_share_last_year\"] = fcfe_per_share_df[\"fcfe_per_share_last_year\"].astype(int, errors = \"ignore\")\n",
    "   # print(fcfe_per_share_df.dtypes)\n",
    "    #get years between first and lasts date \n",
    "    fcfe_per_share_df['fcfe_per_share_years_between'] = fcfe_per_share_df['fcfe_per_share_last_year'] - fcfe_per_share_df['fcfe_per_share_first_year']    \n",
    "    #add ticker back\n",
    "    fcfe_per_share_df.loc[:,'ticker'] = wide_df['ticker']   \n",
    "    fcfe_per_share_df = fcfe_per_share_df[[\n",
    "        'ticker',\n",
    "        'fcfe_per_share_last_value',\n",
    "        'fcfe_per_share_last_year',\n",
    "        'fcfe_per_share_first_value',\n",
    "        'fcfe_per_share_first_year',\n",
    "        'fcfe_per_share_years_avail',\n",
    "        'fcfe_per_share_years_between'\n",
    "    ]]\n",
    "    #print(fcfe_per_share_df)\n",
    "\n",
    "    #join in stats \n",
    "    wide_df = wide_df.merge(revenue_df, on= ['ticker'], how = 'left')\n",
    "    wide_df = wide_df.merge(shares_outstanding_df, on= ['ticker'], how = 'left')\n",
    "    wide_df = wide_df.merge(fcfe_df, on= ['ticker'], how = 'left')\n",
    "    wide_df = wide_df.merge(fcfe_per_share_df, on= ['ticker'], how = 'left')\n",
    "    \n",
    "    #print(wide_df)\n",
    "    \n",
    "    # Calculate FCFE Yields for each year ugh (since price comes in wide)\n",
    "    ######################################################################################################\n",
    "    calc_df = sqldf(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "    *, \n",
    "    (IFNULL(fcfe_yield2013,0) +  IFNULL(fcfe_yield2014,0) +  IFNULL(fcfe_yield2015,0) +  \n",
    "    IFNULL(fcfe_yield2016,0) +  IFNULL(fcfe_yield2017,0) +  IFNULL(fcfe_yield2018,0) +  \n",
    "    IFNULL(fcfe_yield2019,0))/NULLIF(fcfe_per_share_years_avail,0) AS avg_fcfe_yield\n",
    "    FROM \n",
    "    (\n",
    "    SELECT \n",
    "    *, \n",
    "    fcfe_per_share2013/NULLIF(avg_price2013,0) AS fcfe_yield2013,\n",
    "    fcfe_per_share2014/NULLIF(avg_price2014,0) AS fcfe_yield2014,\n",
    "    fcfe_per_share2015/NULLIF(avg_price2015,0) AS fcfe_yield2015,\n",
    "    fcfe_per_share2016/NULLIF(avg_price2016,0) AS fcfe_yield2016,\n",
    "    fcfe_per_share2017/NULLIF(avg_price2017,0) AS fcfe_yield2017, \n",
    "    fcfe_per_share2018/NULLIF(avg_price2018,0) AS fcfe_yield2018,\n",
    "    fcfe_per_share2019/NULLIF(avg_price2019,0) AS fcfe_yield2019 \n",
    "    FROM \n",
    "    wide_df\n",
    "    )\n",
    "    \"\"\"\n",
    "    )\n",
    "    \n",
    "    #get growth measure \n",
    "    calc_df['revenue_CAGR']  = (calc_df['revenue_last_value']/calc_df['revenue_first_value']).fillna(0) ** (1/calc_df['revenue_years_between']).fillna(0) - 1\n",
    "    calc_df['fcfe_per_share_CAGR']  = (calc_df['fcfe_per_share_last_value']/calc_df['fcfe_per_share_first_value']).fillna(0) ** (1/calc_df['fcfe_per_share_years_between']).fillna(0) - 1\n",
    "    \n",
    "    #set growth inhibitor factor \n",
    "    GIF = 0.75\n",
    "    \n",
    "    #get forecast components\n",
    "    #what will market pay for future for fcfe\n",
    "    calc_df['estimated_fcfe_multiplier'] = (1/calc_df['avg_fcfe_yield']).fillna(0) * GIF\n",
    "    #how much will fcfe grow? \n",
    "    calc_df['estimated_fcfe_per_share_CAGR'] = calc_df['fcfe_per_share_CAGR'] * GIF\n",
    "    \n",
    "    \n",
    "    #get estimated future fcfe \n",
    "    calc_df['estimated_fcfe_f1'] = calc_df['fcfe_per_share2019'] * (1+calc_df['estimated_fcfe_per_share_CAGR'])\n",
    "    calc_df['estimated_fcfe_f2'] = calc_df['fcfe_per_share2019'] * (1+calc_df['estimated_fcfe_per_share_CAGR'])**2\n",
    "    calc_df['estimated_fcfe_f3'] = calc_df['fcfe_per_share2019'] * (1+calc_df['estimated_fcfe_per_share_CAGR'])**3\n",
    "    calc_df['estimated_fcfe_f4'] = calc_df['fcfe_per_share2019'] * (1+calc_df['estimated_fcfe_per_share_CAGR'])**4\n",
    "    calc_df['estimated_fcfe_f5'] = calc_df['fcfe_per_share2019'] * (1+calc_df['estimated_fcfe_per_share_CAGR'])**5\n",
    "    calc_df['estimated_fcfe_f6'] = calc_df['fcfe_per_share2019'] * (1+calc_df['estimated_fcfe_per_share_CAGR'])**6\n",
    "    calc_df['estimated_fcfe_f7'] = calc_df['fcfe_per_share2019'] * (1+calc_df['estimated_fcfe_per_share_CAGR'])**7\n",
    "    calc_df['estimated_fcfe_f8'] = calc_df['fcfe_per_share2019'] * (1+calc_df['estimated_fcfe_per_share_CAGR'])**8\n",
    "    calc_df['estimated_fcfe_f9'] = calc_df['fcfe_per_share2019'] * (1+calc_df['estimated_fcfe_per_share_CAGR'])**9\n",
    "    calc_df['estimated_fcfe_f10'] = calc_df['fcfe_per_share2019'] * (1+calc_df['estimated_fcfe_per_share_CAGR'])**10\n",
    "    \n",
    "    #estimated total cash in \n",
    "    calc_df['estimated_sale_price'] = calc_df['estimated_fcfe_f10'] * calc_df['estimated_fcfe_multiplier']\n",
    "    calc_df['estimated_total_cfs'] = calc_df['estimated_sale_price'] - calc_df['current_price'] + calc_df['estimated_fcfe_f1'] + calc_df['estimated_fcfe_f2'] + calc_df['estimated_fcfe_f3'] + calc_df['estimated_fcfe_f4'] + calc_df['estimated_fcfe_f5'] + calc_df['estimated_fcfe_f6'] + calc_df['estimated_fcfe_f7'] + calc_df['estimated_fcfe_f8'] + calc_df['estimated_fcfe_f9'] + calc_df['estimated_fcfe_f10']\n",
    "        \n",
    "    calc_df['estimated_IRR'] = (calc_df['estimated_total_cfs']/calc_df['current_price']) ** (.1) -1     \n",
    "    \n",
    "    \n",
    "    calc_df.set_index(keys= ['ticker'], inplace = True)\n",
    "    ####################################################################################################### \n",
    "    #write to csv\n",
    "    print(\"printing df to write to csv\")\n",
    "    print(calc_df)\n",
    "    calc_df.to_csv(\"sp500_financials.csv\")\n",
    "    return(calc_df)\n",
    "    \n",
    "start = time.time()\n",
    "\n",
    "print(\"starting to process data\")\n",
    "init_df = init_process_data()\n",
    "\n",
    "end = time.time()\n",
    "print(\"finished processing data\")\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in init_df.columns: print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "good_guys = sqldf(\"\"\"\n",
    "SELECT \n",
    "DISTINCT \n",
    "ticker \n",
    "FROM \n",
    "init_df \n",
    "WHERE \n",
    "fcfe_per_share2019 IS NOT NULL \n",
    "AND fcfe_per_share2019 <> 0 \n",
    "AND fcfe2019 <> 0\n",
    "AND fcfe_per_share_years_avail > 2\n",
    "AND fcfe_per_share_years_between > 1\n",
    "      \"\"\")\n",
    "\n",
    "for index, row in good_guys.iterrows():\n",
    "    print(row['ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # section for manual data insertion\n",
    "# df_edit = pd.read_csv(\"sp500_financials.csv\")\n",
    "\n",
    "# #index of dataframe should be ticker: ['ticker', 'colname'] = value\n",
    "\n",
    "# #go thru sp500 alphabetically and set financials you want. \n",
    "\n",
    "# #ticker_mod = 'aapl'\n",
    "# #df_edit.set_value(ticker_mod, 'revenue2013', 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ########################################################################################################################\n",
    "# #write to csv\n",
    "# df_edit.to_csv(\"sp500_financials_editted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess quality of data here \n",
    "pt_good = sqldf(\"\"\"SELECT DISTINCT ticker FROM init_df \n",
    "                    ORDER BY cfo2019 DESC \n",
    "                \"\"\")\n",
    "for index, row in pt_good.iterrows():\n",
    "    print(row['ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
