{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniel\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pandas_datareader\\compat\\__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import datetime as dt \n",
    "import lxml\n",
    "from lxml import html, etree\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader\n",
    "from pandas_datareader import data as pdr\n",
    "import time\n",
    "from time import sleep\n",
    "import string\n",
    "import yfinance as yf\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import io\n",
    "import os\n",
    "import pandasql\n",
    "from pandasql import sqldf\n",
    "import re\n",
    "import gc\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import psutil\n",
    "\n",
    "from collections import Counter\n",
    "import linecache\n",
    "import tracemalloc\n",
    "\n",
    "import reshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\GitHub\\\\finance'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top(snapshot, key_type='lineno', limit=3):\n",
    "    snapshot = snapshot.filter_traces((\n",
    "        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "        tracemalloc.Filter(False, \"<unknown>\"),\n",
    "    ))\n",
    "    top_stats = snapshot.statistics(key_type)\n",
    "\n",
    "    print(\"Top %s lines\" % limit)\n",
    "    for index, stat in enumerate(top_stats[:limit], 1):\n",
    "        frame = stat.traceback[0]\n",
    "        # replace \"/path/to/module/file.py\" with \"module/file.py\"\n",
    "        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\n",
    "        print(\"#%s: %s:%s: %.1f KiB\"\n",
    "              % (index, filename, frame.lineno, stat.size / 1024))\n",
    "        line = linecache.getline(frame.filename, frame.lineno).strip()\n",
    "        if line:\n",
    "            print('    %s' % line)\n",
    "\n",
    "    other = top_stats[limit:]\n",
    "    if other:\n",
    "        size = sum(stat.size for stat in other)\n",
    "        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\n",
    "    total = sum(stat.size for stat in top_stats)\n",
    "    print(\"Total allocated size: %.1f KiB\" % (total / 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ticker      cik\n",
      "0      aal     6201\n",
      "1      aap  1158449\n",
      "2     aapl   320193\n",
      "3     abbv  1551152\n",
      "4      abc  1140859\n",
      "..     ...      ...\n",
      "495    yum  1041061\n",
      "496    zbh  1136869\n",
      "497   zbra   877212\n",
      "498   zion   109380\n",
      "499    zts  1555280\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#get ticker, company name and CIK numbers \n",
    "ticker_url = \"https://www.sec.gov/include/ticker.txt\"\n",
    "ticker_request = requests.get(ticker_url).content\n",
    "ticker_df = pd.read_csv(io.StringIO(ticker_request.decode('utf-8')),sep=\"\\t\")\n",
    "ticker_df.columns = ['ticker', 'cik']\n",
    "ticker_df[\"ticker\"] = ticker_df[\"ticker\"].str.lower()\n",
    "\n",
    "\n",
    "#get sp500 tickers \n",
    "sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "sp500 = pd.DataFrame(sp500[0]['Symbol'])\n",
    "sp500.columns = ['ticker']\n",
    "sp500[\"ticker\"] = sp500[\"ticker\"].str.lower()\n",
    "#print(sp500)\n",
    "\n",
    "#get cik of sp 500\n",
    "sample = pd.merge(ticker_df,sp500,on='ticker')\n",
    "print(sample )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data fetched for symbol blk using YahooDailyReader\n",
      "blk failed\n",
      "'Date'\n",
      "br failed\n",
      "No data fetched for symbol cb using YahooDailyReader\n",
      "cb failed\n",
      "'Date'\n",
      "cboe failed\n",
      "No data fetched for symbol cbre using YahooDailyReader\n",
      "cbre failed\n",
      "'Date'\n",
      "otis failed\n",
      "        avg_price2013  avg_price2014  avg_price2015  avg_price2016  \\\n",
      "ticker                                                               \n",
      "aal         17.146522      36.755626      42.940213      36.716199   \n",
      "aap         83.696200     128.910822     161.370373     153.556964   \n",
      "aapl        59.771992      83.652771     110.696663      98.462125   \n",
      "abbv        32.361064      42.512583      49.072086      49.757348   \n",
      "abc         50.829051      67.690750      95.513013      77.419414   \n",
      "...               ...            ...            ...            ...   \n",
      "yum         43.882041      47.608164      52.553995      55.586190   \n",
      "zbh         75.556204      96.441867     104.284561     109.899370   \n",
      "zbra        46.445833      71.196667      89.878492      64.032024   \n",
      "zion        24.429700      26.550984      26.060497      26.514911   \n",
      "zts         30.203858      32.603061      44.496182      46.556359   \n",
      "\n",
      "        avg_price2017  avg_price2018  avg_price2019  current_price  \n",
      "ticker                                                              \n",
      "aal         46.100999      41.898407      30.571997             11  \n",
      "aap        121.234932     138.043532     157.406087            147  \n",
      "aapl       144.384174     184.072185     205.968825            370  \n",
      "abbv        64.069739      85.806370      72.340373             97  \n",
      "abc         81.515308      85.814323      81.279619            103  \n",
      "...               ...            ...            ...            ...  \n",
      "yum         68.758763      81.697489     102.398097             93  \n",
      "zbh        115.598098     115.463559     127.277667            134  \n",
      "zbra        99.310319     150.661314     206.708492            269  \n",
      "zion        41.197410      49.195448      44.921350             33  \n",
      "zts         59.965932      84.415576     109.644249            144  \n",
      "\n",
      "[494 rows x 8 columns]\n",
      "finished getting easy stats for sp500\n",
      "1872.5792634487152\n"
     ]
    }
   ],
   "source": [
    "#get tickers in the sp 500 and get easy stats on them\n",
    "#get avg price by year, and current price of stock \n",
    "def get_easy_stats(df):\n",
    "    \n",
    "    easy_df = []\n",
    "    \n",
    "    start_dt = dt.datetime(2013,1,1)\n",
    "    end_dt = dt.datetime(2019,12,31)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        try: \n",
    "            sleep(1)\n",
    "            data_ts = pdr.get_data_yahoo(row['ticker'], start_dt, end_dt)\n",
    "            data_ts['year'] = data_ts.index.year\n",
    "            #get average price per share \n",
    "            agg_df = data_ts.groupby('year', as_index=False)['Adj Close'].mean()\n",
    "            agg_df['ticker'] = row['ticker']\n",
    "            agg_df.columns = ['year','avg_price', 'ticker']\n",
    "            agg_df['year']=  agg_df['year'].astype(str)\n",
    "    #       print(agg_df)\n",
    "            current_price = pdr.get_data_yahoo(row['ticker']).last('1D')['Adj Close'][0].astype(int)\n",
    "\n",
    "            #turn agg_df wide  for price rows\n",
    "            wide_df = agg_df.pivot(index='ticker', columns='year', values=['avg_price'])\n",
    "\n",
    "            wide_df.columns = list(map(\"\".join, wide_df.columns))\n",
    "            wide_df['current_price'] = current_price\n",
    "            #print(wide_df)\n",
    "\n",
    "            easy_df.append(wide_df)\n",
    "            del agg_df\n",
    "            del wide_df\n",
    "        \n",
    "        except Exception as ex:\n",
    "            print(str(ex))\n",
    "            print(row['ticker'] + \" failed\")\n",
    "    \n",
    "    easy_df = pd.concat(easy_df)\n",
    "    easy_df.to_csv(\"easy_stats_sp500.csv\")\n",
    "    return(easy_df)\n",
    "#function end\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#run to generate easy stat csv\n",
    "easy_stat_df = get_easy_stats(sample)\n",
    "print(easy_stat_df)\n",
    "\n",
    "end = time.time()\n",
    "print(\"finished getting easy stats for sp500\")\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ticker  avg_price2013  avg_price2014  avg_price2015  avg_price2016  \\\n",
      "0      aal      17.146522      36.755626      42.940213      36.716199   \n",
      "1      aap      83.696200     128.910822     161.370373     153.556964   \n",
      "2     aapl      59.771992      83.652771     110.696663      98.462125   \n",
      "3     abbv      32.361064      42.512583      49.072086      49.757348   \n",
      "4      abc      50.829051      67.690750      95.513013      77.419414   \n",
      "..     ...            ...            ...            ...            ...   \n",
      "489    yum      43.882041      47.608164      52.553995      55.586190   \n",
      "490    zbh      75.556204      96.441867     104.284561     109.899370   \n",
      "491   zbra      46.445833      71.196667      89.878492      64.032024   \n",
      "492   zion      24.429700      26.550984      26.060497      26.514911   \n",
      "493    zts      30.203858      32.603061      44.496182      46.556359   \n",
      "\n",
      "     avg_price2017  avg_price2018  avg_price2019  current_price  \n",
      "0        46.100999      41.898407      30.571997             11  \n",
      "1       121.234932     138.043532     157.406087            147  \n",
      "2       144.384174     184.072185     205.968825            370  \n",
      "3        64.069739      85.806370      72.340373             97  \n",
      "4        81.515308      85.814323      81.279619            103  \n",
      "..             ...            ...            ...            ...  \n",
      "489      68.758763      81.697489     102.398097             93  \n",
      "490     115.598098     115.463559     127.277667            134  \n",
      "491      99.310319     150.661314     206.708492            269  \n",
      "492      41.197410      49.195448      44.921350             33  \n",
      "493      59.965932      84.415576     109.644249            144  \n",
      "\n",
      "[494 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "#read in the easy stats of sp500\n",
    "easy_stat_df = pd.read_csv(\"easy_stats_sp500.csv\")\n",
    "print(easy_stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up script to traverse directory of quarterly submissions\n",
    "def generate_finance_table_csv():\n",
    "\n",
    "    #store all finances in a table\n",
    "    finance_list = []\n",
    "\n",
    "    #go into directory with data\n",
    "    file_os = 'D:\\\\Finance Data'\n",
    "    \n",
    "    \n",
    "    code_os = 'D:\\\\GitHub\\\\finance'\n",
    "    start = time.time()\n",
    "    \n",
    "    #go into file directory\n",
    "    os.chdir(file_os)\n",
    "    \n",
    "    for i in os.listdir():\n",
    "        file_path = i\n",
    "        #if its a quarterly filing data, parse it\n",
    "        if 'q1' in file_path or 'q2' in file_path or 'q3' in file_path or 'q4' in file_path:\n",
    "            try:\n",
    "                tracemalloc.start()\n",
    "                print(\"starting: \" + file_path)\n",
    "\n",
    "                sub = pd.read_csv(file_path + '/' + 'sub.txt', sep = '\\t', encoding = \"ISO-8859-1\", iterator = True, chunksize =100000)\n",
    "                sub = pd.concat(sub)\n",
    "                #clean up form column and filter to just 10-K submissions\n",
    "                sub['form'] = sub['form'].replace(np.nan, '', regex=True)\n",
    "                sub = sub[sub['form'] == '10-K']\n",
    "                #filter to the s&p500\n",
    "                sub = pd.merge(sub,sample, on = ['cik'])\n",
    "\n",
    "                pre = pd.read_csv(file_path + '/' + 'pre.txt', sep = '\\t', encoding = \"ISO-8859-1\", chunksize =1000000) \n",
    "                num = pd.read_csv(file_path + '/' + 'num.txt', sep = '\\t', encoding = \"ISO-8859-1\",  chunksize =100000)\n",
    "\n",
    "                print(\"joining num to sub\")\n",
    "                num_sub = pd.DataFrame()              \n",
    "                #join in numbers to submissions in chunks\n",
    "                for chunks in num: \n",
    "                    #print(chunks)\n",
    "                    num_sub = pd.concat([num_sub, sub.merge(chunks, on=['adsh'])])\n",
    "\n",
    "                #no longer need sub\n",
    "                del sub\n",
    "\n",
    "                print(\"joining pre to numsub\")\n",
    "                nsp = pd.DataFrame()\n",
    "                #add in plabel and stmt info\n",
    "                for chunks in pre:\n",
    "                    #print(chunks)\n",
    "                    nsp = pd.concat([nsp, num_sub.merge(chunks, on=['adsh', 'tag', 'version'])])\n",
    "                    nsp = nsp[nsp['stmt'].isin(['BS', 'IS', 'CF', 'CI', 'EQ'])]\n",
    "                # delete files no longer needed\n",
    "                del num_sub\n",
    "                del pre\n",
    "                del num\n",
    "\n",
    "\n",
    "                #print(nsp.columns)\n",
    "\n",
    "                nsp = nsp[['name', 'sic', 'fye', 'form', 'period', \n",
    "                           'fy', 'fp', 'filed','ticker', 'cik','ddate', \n",
    "                           'qtrs', 'uom', 'value', 'adsh','stmt',  'tag', \n",
    "                           'version', 'plabel']]\n",
    "\n",
    "                #add finances to master table\n",
    "                finance_list.append(nsp)\n",
    "                del nsp\n",
    "                print(\"processed_data: \" + file_path)\n",
    "                gc.collect()\n",
    "                snapshot = tracemalloc.take_snapshot()\n",
    "                display_top(snapshot)\n",
    "            except Exception as ex:\n",
    "                print(\"failed: \" + file_path)\n",
    "                print(str(ex))\n",
    "    \n",
    "    #switch back to github directory \n",
    "    os.chdir(code_os)\n",
    "    \n",
    "    \n",
    "    finance_table = pd.concat(finance_list)\n",
    "    del finance_list \n",
    "    finance_table.to_csv(\"total_finance_table.csv\")\n",
    "    end = time.time()\n",
    "    print(\"finished traversing files in X secs\")\n",
    "    print(end - start)\n",
    "    #return(finance_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting: 2014q1\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2014q1\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 25078.3 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 14015.1 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: internals\\managers.py:1848: 7388.1 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "168 other: 1845.4 KiB\n",
      "Total allocated size: 48327.0 KiB\n",
      "starting: 2014q2\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2014q2\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 26137.8 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 16286.1 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: internals\\managers.py:1848: 11092.2 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "224 other: 2397.1 KiB\n",
      "Total allocated size: 55913.2 KiB\n",
      "starting: 2014q3\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2014q3\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 27837.1 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 11634.5 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: internals\\managers.py:1848: 4959.4 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "228 other: 2541.8 KiB\n",
      "Total allocated size: 46972.8 KiB\n",
      "starting: 2014q4\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2014q4\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 29581.6 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 8307.2 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: internals\\managers.py:1848: 2949.3 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "231 other: 2695.4 KiB\n",
      "Total allocated size: 43533.5 KiB\n",
      "starting: 2015q1\n",
      "joining num to sub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniel\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3254: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joining pre to numsub\n",
      "processed_data: 2015q1\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 55402.8 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 15638.9 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: internals\\managers.py:1848: 6313.8 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "232 other: 4782.5 KiB\n",
      "Total allocated size: 82137.9 KiB\n",
      "starting: 2015q2\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2015q2\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 55495.7 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 10073.6 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 3963.8 KiB\n",
      "    taken = self.values.take(indices)\n",
      "235 other: 3419.6 KiB\n",
      "Total allocated size: 72952.7 KiB\n",
      "starting: 2015q3\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2015q3\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 57183.8 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 9091.4 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 4084.3 KiB\n",
      "    taken = self.values.take(indices)\n",
      "238 other: 2972.9 KiB\n",
      "Total allocated size: 73332.5 KiB\n",
      "starting: 2015q4\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2015q4\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 59113.9 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 8359.3 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 4222.1 KiB\n",
      "    taken = self.values.take(indices)\n",
      "242 other: 2409.8 KiB\n",
      "Total allocated size: 74105.1 KiB\n",
      "starting: 2016q1\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2016q1\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 84907.6 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 13804.0 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 6064.5 KiB\n",
      "    taken = self.values.take(indices)\n",
      "250 other: 4020.1 KiB\n",
      "Total allocated size: 108796.2 KiB\n",
      "starting: 2016q2\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2016q2\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 85718.1 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 53573.3 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 40257.2 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "249 other: 7352.7 KiB\n",
      "Total allocated size: 186901.3 KiB\n",
      "starting: 2016q3\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2016q3\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 87245.8 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 52292.6 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 32746.7 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "248 other: 7482.1 KiB\n",
      "Total allocated size: 179767.1 KiB\n",
      "starting: 2016q4\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2016q4\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 89000.2 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 52132.4 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 33331.4 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "248 other: 7629.5 KiB\n",
      "Total allocated size: 182093.5 KiB\n",
      "starting: 2017q1\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2017q1\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 114072.8 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 12495.9 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 8146.7 KiB\n",
      "    taken = self.values.take(indices)\n",
      "251 other: 1824.0 KiB\n",
      "Total allocated size: 136539.5 KiB\n",
      "starting: 2017q2\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2017q2\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 115046.2 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 54597.6 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 36566.6 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "251 other: 9518.2 KiB\n",
      "Total allocated size: 215728.6 KiB\n",
      "starting: 2017q3\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2017q3\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 116554.6 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 49712.9 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 36424.2 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "251 other: 9639.5 KiB\n",
      "Total allocated size: 212331.2 KiB\n",
      "starting: 2017q4\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2017q4\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 118199.1 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 50698.6 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 38380.8 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "252 other: 9779.6 KiB\n",
      "Total allocated size: 217058.1 KiB\n",
      "starting: 2018q1\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2018q1\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 140097.3 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 52831.8 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 39092.9 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "263 other: 11354.2 KiB\n",
      "Total allocated size: 243376.2 KiB\n",
      "starting: 2018q2\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2018q2\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 141168.1 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 20639.1 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 10081.8 KiB\n",
      "    taken = self.values.take(indices)\n",
      "272 other: 4546.8 KiB\n",
      "Total allocated size: 176435.9 KiB\n",
      "starting: 2018q3\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2018q3\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 142492.8 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 50380.8 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 37908.0 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "270 other: 11550.2 KiB\n",
      "Total allocated size: 242331.7 KiB\n",
      "starting: 2018q4\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2018q4\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 144292.4 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 51361.2 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 39174.2 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "269 other: 11693.0 KiB\n",
      "Total allocated size: 246520.8 KiB\n",
      "starting: 2019q1\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2019q1\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 164940.3 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 52620.4 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 42136.6 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "272 other: 13172.9 KiB\n",
      "Total allocated size: 272870.2 KiB\n",
      "starting: 2019q2\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2019q2\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 166160.6 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 27967.6 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 11866.6 KiB\n",
      "    taken = self.values.take(indices)\n",
      "273 other: 7893.6 KiB\n",
      "Total allocated size: 213888.4 KiB\n",
      "starting: 2019q3\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2019q3\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 167615.1 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: internals\\managers.py:1848: 54016.3 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#3: io\\parsers.py:2037: 44826.4 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "276 other: 13388.6 KiB\n",
      "Total allocated size: 279846.4 KiB\n",
      "starting: 2019q4\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2019q4\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 169307.5 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 22470.8 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 12091.0 KiB\n",
      "    taken = self.values.take(indices)\n",
      "276 other: 3799.4 KiB\n",
      "Total allocated size: 207668.7 KiB\n",
      "starting: 2020q1\n",
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2020q1\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 190638.8 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 25877.6 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 13614.7 KiB\n",
      "    taken = self.values.take(indices)\n",
      "279 other: 4428.1 KiB\n",
      "Total allocated size: 234559.2 KiB\n",
      "starting: 2020q2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-41b3537bcb97>:2: DtypeWarning: Columns (35) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  generate_finance_table_csv()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joining num to sub\n",
      "joining pre to numsub\n",
      "processed_data: 2020q2\n",
      "Top 3 lines\n",
      "#1: core\\algorithms.py:1657: 191544.2 KiB\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "#2: io\\parsers.py:2037: 30056.2 KiB\n",
      "    data = self._reader.read(nrows)\n",
      "#3: indexes\\base.py:763: 13679.2 KiB\n",
      "    taken = self.values.take(indices)\n",
      "287 other: 6751.7 KiB\n",
      "Total allocated size: 242031.3 KiB\n",
      "finished traversing files in X secs\n",
      "632.563866853714\n"
     ]
    }
   ],
   "source": [
    "# #run function to generate finance table csv\n",
    "generate_finance_table_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function goes through and tries to parse out the historical data we need: VERY LONG \n",
    "def parse_data(df):\n",
    "    gc.collect()\n",
    "    ############################################################\n",
    "    #process data a bit \n",
    "    \n",
    "    #lower case plabel \n",
    "    df['plabel'] = df['plabel'].str.lower()\n",
    "    \n",
    "    #turn things into an integer\n",
    "    df['qtrs'] = df.qtrs.astype(int)\n",
    "    df['period'] = df.period.astype(int)\n",
    "    df['fy'] = df.fy.astype(int)\n",
    "    df['fye'] = df.fye.astype(int)\n",
    "    df['ddate'] = df.ddate.astype(int)\n",
    "    df['ddate_prev'] = df.ddate.astype(int) + 10000\n",
    "    \n",
    "    #turn things to strings\n",
    "    df['period'] = df['period'].astype(str)\n",
    "    df['fye'] = df['fye'].astype(str)\n",
    "    df['fy'] = df['fy'].astype(str)\n",
    "    df['ddate'] = df['ddate'].astype(str)\n",
    "    df['ddate_prev'] = df['ddate_prev'].astype(str)\n",
    "    #pad fye with a leading 0 \n",
    "    df['fye'] = df['fye'].apply(lambda x: x.zfill(4))\n",
    "    \n",
    "    #fill nas inplace = true changes the df directly\n",
    "    df['period'].fillna('', inplace=True)\n",
    "    df['fy'].fillna('', inplace=True)\n",
    "    df['fye'].fillna('', inplace=True)\n",
    "    df['plabel'].fillna('', inplace=True)\n",
    "    df['ddate'].fillna('', inplace=True)\n",
    "    df['ddate_prev'].fillna('', inplace=True)\n",
    "    \n",
    "    #create a joinable key\n",
    "    df['id'] = df['fy'].astype(str) + '_' + df['cik'].astype(str)\n",
    "\n",
    "    #get distinct company and years\n",
    "    key_df = pd.DataFrame(df.id.unique(), columns = ['id'])\n",
    "    gc.collect()\n",
    "    \n",
    "    #######################################################################################################\n",
    "    #get annual revenue \n",
    "    revenue_rows = df[\n",
    "        # get yearly cash flow statements\n",
    "        (df.stmt.str.contains('IS',na=False))\n",
    "        & (df.qtrs == 4)\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('revenue',na=False) | df.plabel.str.contains('sales',na=False)) \n",
    "    ]\n",
    "    #get max value for each company and fiscal year\n",
    "    revenue_agg = revenue_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    revenue_agg.columns = ['id', 'revenue']\n",
    "    #print(revenue_agg)\n",
    "    del revenue_rows\n",
    "    \n",
    "    #get annual cash flow from operations rows \n",
    "    cfo_rows = df[\n",
    "        # get yearly cash flow statements\n",
    "        (df.stmt.str.contains('CF',na=False))\n",
    "        & (df.qtrs == 4)\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('cash',na=False)) \n",
    "        & (df.plabel.str.contains('operating',na=False)) \n",
    "        ### exclusion words #####\n",
    "        & (~df.plabel.str.contains('discontinue', na=False))\n",
    "        & (~df.plabel.str.contains('cost', na=False))\n",
    "        & (~df.plabel.str.contains('lease', na=False))\n",
    "        & (~df.plabel.str.contains('adjustments', na=False))\n",
    "        & (~df.plabel.str.contains('non-cash', na=False))\n",
    "        & (~df.plabel.str.contains('other', na=False))\n",
    "        & (~df.plabel.str.contains('disposal', na=False))\n",
    "        & (~df.plabel.str.contains('escrow', na=False))\n",
    "        & (~df.plabel.str.contains('investing', na=False))\n",
    "        & (~df.plabel.str.contains('partnership', na=False))\n",
    "        & (~df.plabel.str.contains('restricted', na=False))\n",
    "        & (~df.plabel.str.contains('securities', na=False))\n",
    "    ]\n",
    "    #get max value for each company and fiscal year\n",
    "    cfo_agg = cfo_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    cfo_agg.columns = ['id', 'cfo']\n",
    "    #print(cfo_agg)\n",
    "    del cfo_rows\n",
    "    \n",
    "    #get annual shares outstanding \n",
    "    shares_outstanding_rows = df[\n",
    "        #get yearly income stmt \n",
    "        (df.stmt.str.contains('IS',na=False))\n",
    "       # & (df.qtrs == 4)\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.uom.str.contains('shares',na=False))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('shares',na=False) | df.plabel.str.contains('stock',na=False) )\n",
    "        ### exclusion words\n",
    "    ]\n",
    "    #get max number of shares outstandiing from income statement  for each company and fiscal year\n",
    "    shares_outstanding_agg = shares_outstanding_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    shares_outstanding_agg.columns = ['id', 'shares_outstanding']\n",
    "    del shares_outstanding_rows\n",
    "    \n",
    "    #get current assets \n",
    "    current_assets_rows = df[\n",
    "        #get yearly balance sheet statements\n",
    "        (df.stmt.str.contains('BS',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('current',na=False)) \n",
    "        & (df.plabel.str.contains('asset',na=False)) \n",
    "    ]\n",
    "    current_assets_agg = current_assets_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    current_assets_agg.columns = ['id', 'current_assets']\n",
    "    del current_assets_rows\n",
    "    \n",
    "    #get current liabilites\n",
    "    current_liabilities_rows = df[\n",
    "        #get yearly balance sheet statements\n",
    "        (df.stmt.str.contains('BS',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('current',na=False)) \n",
    "        & (df.plabel.str.contains('liabilities',na=False)) \n",
    "    ]\n",
    "    current_liabilities_agg = current_liabilities_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    current_liabilities_agg.columns = ['id', 'current_liabilities']\n",
    "    del current_liabilities_rows\n",
    "    \n",
    "    #get depreciation and amortization\n",
    "    dep_amort_rows = df[\n",
    "        (df.stmt.str.contains('CF',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('depreciation',na=False)) \n",
    "        & (df.plabel.str.contains('amortization',na=False)) \n",
    "    ]\n",
    "    dep_amort_agg = dep_amort_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    dep_amort_agg.columns = ['id', 'dep_amort']\n",
    "    del dep_amort_rows\n",
    "    \n",
    "    # try to get depreciation only \n",
    "    dep_only_rows = df[\n",
    "        (df.stmt.str.contains('CF',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('depreciation',na=False)) \n",
    "    ]\n",
    "    dep_only_agg = dep_only_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    dep_only_agg.columns = ['id', 'dep_only']\n",
    "    del dep_only_rows\n",
    "    \n",
    "    # try to get amort only\n",
    "    amort_only_rows = df[\n",
    "        #get yearly balance sheet statements\n",
    "        (df.stmt.str.contains('CF',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('amortization',na=False)) \n",
    "    ]\n",
    "    amort_only_agg = amort_only_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    amort_only_agg.columns = ['id', 'amort_only']\n",
    "    del amort_only_rows\n",
    "    \n",
    "    #get PPE \n",
    "    ppe_rows = df[\n",
    "        #get yearly income statements\n",
    "        (df.stmt.str.contains('BS',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('property',na=False)) \n",
    "        & (df.plabel.str.contains('equipment',na=False)) \n",
    "    ]\n",
    "    ppe_agg = ppe_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    ppe_agg.columns = ['id', 'ppe']\n",
    "    del ppe_rows\n",
    "    \n",
    "    #get PPE of the year before\n",
    "    ppe_prev_rows = df[\n",
    "        (df.stmt.str.contains('BS',na=False))\n",
    "        & (df['ddate_prev'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('property',na=False)) \n",
    "        & (df.plabel.str.contains('equipment',na=False)) \n",
    "    ]\n",
    "    ppe_prev_agg = ppe_prev_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    ppe_prev_agg.columns = ['id', 'ppe_prev']\n",
    "    del ppe_prev_rows\n",
    "    \n",
    "    #get operating income \n",
    "    operating_income_rows = df[\n",
    "        #get yearly income stmt \n",
    "        (df.stmt.str.contains('IS',na=False))\n",
    "        & (df.qtrs == 4)\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('operating',na=False))\n",
    "        & (df.plabel.str.contains('income',na=False) )\n",
    "        ### exclusion words\n",
    "    ]\n",
    "    operating_income_agg = operating_income_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    operating_income_agg.columns = ['id', 'operating_income']\n",
    "    del operating_income_rows\n",
    "    \n",
    "    #get long term debt on balance sheet\n",
    "    lt_debt_rows = df[\n",
    "        (df.stmt.str.contains('BS',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('debt',na=False)) \n",
    "    ]\n",
    "    lt_debt_agg = lt_debt_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    lt_debt_agg.columns = ['id', 'lt_debt']\n",
    "    del lt_debt_rows\n",
    "    \n",
    "    #get short term debt on balance sheet\n",
    "    st_debt_rows = df[\n",
    "        (df.stmt.str.contains('BS',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('st_debt',na=False)) \n",
    "    ]\n",
    "    st_debt_agg = st_debt_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    st_debt_agg.columns = ['id', 'st_debt']\n",
    "    del st_debt_rows\n",
    "    \n",
    "    #get short term debt proceeds from cash flow stmt\n",
    "    st_debt_proceeds_rows = df[\n",
    "        (df.stmt.str.contains('CF',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('short',na=False))\n",
    "        & (df.plabel.str.contains('debt',na=False)) \n",
    "        & (df.plabel.str.contains('proceed',na=False))\n",
    "    ]\n",
    "    st_debt_proceeds_agg = st_debt_proceeds_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    st_debt_proceeds_agg.columns = ['id', 'st_debt_proceeds']\n",
    "    del st_debt_proceeds_rows\n",
    "    \n",
    "    #get long term debt proceeds  from cash flow stmt\n",
    "    lt_debt_proceeds_rows = df[\n",
    "        (df.stmt.str.contains('CF',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "        #inclusion words\n",
    "        &\n",
    "        (\n",
    "            (\n",
    "                (df.plabel.str.contains('long',na=False))\n",
    "                & (df.plabel.str.contains('debt',na=False)) \n",
    "                & (df.plabel.str.contains('proceed',na=False))\n",
    "            ) | \n",
    "            (\n",
    "            \n",
    "                (df.plabel.str.contains('issuance',na=False)) \n",
    "                & (df.plabel.str.contains('senior notes',na=False))\n",
    "                & (df.plabel.str.contains('proceed',na=False))\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    lt_debt_proceeds_agg = lt_debt_proceeds_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    lt_debt_proceeds_agg.columns = ['id', 'lt_debt_proceeds']\n",
    "    del lt_debt_proceeds_rows\n",
    "    \n",
    "    #get short term debt repayments from cash flow stmt\n",
    "    st_debt_repayments_rows = df[\n",
    "        (df.stmt.str.contains('CF',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('short',na=False))\n",
    "        & (df.plabel.str.contains('debt',na=False)) \n",
    "        & (df.plabel.str.contains('repayment',na=False))\n",
    "    ]\n",
    "    st_debt_repayments_agg = st_debt_repayments_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    st_debt_repayments_agg.columns = ['id', 'st_debt_repayments']\n",
    "    del st_debt_repayments_rows\n",
    "    \n",
    "    #get long term debt repayments  from cash flow stmt\n",
    "    lt_debt_repayments_rows = df[\n",
    "        (df.stmt.str.contains('CF',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        & (df.qtrs == 4)\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('long',na=False))\n",
    "        & (df.plabel.str.contains('debt',na=False)) \n",
    "        & (df.plabel.str.contains('repayment',na=False))\n",
    "    ]\n",
    "    lt_debt_repayments_agg = lt_debt_repayments_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    lt_debt_repayments_agg.columns = ['id', 'lt_debt_repayments']\n",
    "    del lt_debt_repayments_rows\n",
    "    \n",
    "    #try to get capital expenditure from cash flows \n",
    "    \n",
    "    #get cash and cash equivalents\n",
    "    cash_rows = df[\n",
    "        (df.stmt.str.contains('BS',na=False))\n",
    "        & (df['ddate'] == (df['fy'] + df['fye']))\n",
    "        #inclusion words\n",
    "        & (df.plabel.str.contains('cash',na=False)) \n",
    "        ### exclusion words #####\n",
    "        & (~df.plabel.str.contains('discontinue', na=False))\n",
    "        & (~df.plabel.str.contains('cost', na=False))\n",
    "        & (~df.plabel.str.contains('lease', na=False))\n",
    "        & (~df.plabel.str.contains('adjustments', na=False))\n",
    "        & (~df.plabel.str.contains('other', na=False))\n",
    "        & (~df.plabel.str.contains('disposal', na=False))\n",
    "        & (~df.plabel.str.contains('escrow', na=False))\n",
    "        & (~df.plabel.str.contains('investing', na=False))\n",
    "        & (~df.plabel.str.contains('partnership', na=False))\n",
    "        & (~df.plabel.str.contains('restricted', na=False))\n",
    "        & (~df.plabel.str.contains('securities', na=False))\n",
    "    ]\n",
    "    cash_agg = cash_rows.groupby(['id'], as_index=False)['value'].max()  \n",
    "    cash_agg.columns = ['id', 'cash']\n",
    "    del cash_rows\n",
    "    \n",
    "    #######################################################################################################\n",
    "    #add financials to key_df\n",
    "    merged_df = key_df.merge(cfo_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(revenue_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(shares_outstanding_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(current_assets_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(current_liabilities_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(dep_amort_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(dep_only_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(amort_only_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(ppe_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(ppe_prev_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(operating_income_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(lt_debt_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(st_debt_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(lt_debt_proceeds_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(st_debt_proceeds_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(lt_debt_repayments_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(st_debt_repayments_agg, on = ['id'], how = \"left\")\n",
    "    merged_df = merged_df.merge(cash_agg, on = ['id'], how = \"left\")\n",
    "    \n",
    "    \n",
    "    #######################################################################################################\n",
    "    #delete unneeded things \n",
    "    del cfo_agg\n",
    "    del revenue_agg\n",
    "    del shares_outstanding_agg\n",
    "    del current_assets_agg\n",
    "    del current_liabilities_agg\n",
    "    del dep_amort_agg\n",
    "    del dep_only_agg\n",
    "    del amort_only_agg\n",
    "    del ppe_agg\n",
    "    del ppe_prev_agg\n",
    "    del operating_income_agg\n",
    "    del lt_debt_agg\n",
    "    del st_debt_agg\n",
    "    del lt_debt_proceeds_agg\n",
    "    del st_debt_proceeds_agg\n",
    "    del lt_debt_repayments_agg\n",
    "    del st_debt_repayments_agg\n",
    "    del cash_agg \n",
    "    \n",
    "    \n",
    "    #print(merged_df)\n",
    "    #print(merged_df.columns)\n",
    "    return(merged_df)\n",
    "    #print(key_df)\n",
    "\n",
    "#     return(result)\n",
    "    #get all submissions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to process data\n",
      "printing pre-aggregated data\n",
      "               id           cfo       revenue  shares_outstanding  \\\n",
      "0       2013_4904  4.106000e+09  1.535700e+10        4.870410e+08   \n",
      "1       2013_4977  1.054700e+10  2.393900e+10        4.674080e+08   \n",
      "2       2013_5513  1.031500e+09  1.035380e+10                 NaN   \n",
      "3       2013_6201  9.490000e+08           NaN        2.763260e+08   \n",
      "4       2013_7084           NaN  8.980400e+10        6.630000e+08   \n",
      "..            ...           ...           ...                 ...   \n",
      "414  2019_1688568  1.783000e+09  2.075300e+10                 NaN   \n",
      "415  2019_1757898  5.395050e+08  2.782170e+09                 NaN   \n",
      "416    2019_56873  4.164000e+09  1.218520e+11        8.180000e+08   \n",
      "417   2019_849399  1.495000e+09  2.456000e+09        6.320000e+08   \n",
      "418  2020_1613103  7.234000e+09  2.891300e+10        1.351100e+09   \n",
      "\n",
      "     current_assets  current_liabilities     dep_amort      dep_only  \\\n",
      "0      4.310000e+09         3.421600e+10           NaN           NaN   \n",
      "1               NaN                  NaN           NaN           NaN   \n",
      "2               NaN                  NaN           NaN           NaN   \n",
      "3      1.432300e+10         3.120300e+10  1.020000e+09  1.020000e+09   \n",
      "4      2.853000e+10         1.565800e+10  9.090000e+08  9.090000e+08   \n",
      "..              ...                  ...           ...           ...   \n",
      "414    9.066000e+09         9.453000e+09  2.023000e+09  2.023000e+09   \n",
      "415    1.053735e+09         4.651960e+08  2.259210e+08  2.259210e+08   \n",
      "416    1.080300e+10         1.427400e+10  2.465000e+09  2.465000e+09   \n",
      "417    3.186000e+09         3.766000e+09  6.150000e+08  6.150000e+08   \n",
      "418    2.203100e+10         1.036600e+10  2.663000e+09  2.663000e+09   \n",
      "\n",
      "       amort_only           ppe  ...  operating_income       lt_debt  st_debt  \\\n",
      "0             NaN  6.028500e+10  ...      2.855000e+09  1.682800e+10      NaN   \n",
      "1             NaN  4.810000e+08  ...               NaN           NaN      NaN   \n",
      "2             NaN  5.119000e+08  ...               NaN  7.650000e+07      NaN   \n",
      "3    1.020000e+09  1.925900e+10  ...      1.534000e+09           NaN      NaN   \n",
      "4    9.090000e+08  2.353000e+10  ...               NaN  5.347000e+09      NaN   \n",
      "..            ...           ...  ...               ...           ...      ...   \n",
      "414  2.023000e+09  3.179000e+09  ...               NaN  5.470000e+09      NaN   \n",
      "415  2.259210e+08  1.031582e+09  ...               NaN  1.183227e+09      NaN   \n",
      "416  2.465000e+09  2.163500e+10  ...               NaN  1.207200e+10      NaN   \n",
      "417  6.150000e+08  6.630000e+08  ...      1.580000e+08  3.961000e+09      NaN   \n",
      "418  2.663000e+09  4.828000e+09  ...      3.560000e+08  2.202100e+10      NaN   \n",
      "\n",
      "     lt_debt_proceeds  st_debt_proceeds  lt_debt_repayments  \\\n",
      "0                 NaN               NaN                 NaN   \n",
      "1                 NaN               NaN                 NaN   \n",
      "2                 NaN               NaN         116200000.0   \n",
      "3        5.134000e+09               NaN                 NaN   \n",
      "4                 NaN               NaN                 NaN   \n",
      "..                ...               ...                 ...   \n",
      "414               NaN               NaN                 NaN   \n",
      "415               NaN               NaN          85000000.0   \n",
      "416      2.236000e+09               NaN                 NaN   \n",
      "417               NaN               NaN                 NaN   \n",
      "418               NaN               NaN                 NaN   \n",
      "\n",
      "     st_debt_repayments          cash  year      cik  \n",
      "0                   NaN  1.180000e+08  2013     4904  \n",
      "1                   NaN  2.543000e+09  2013     4977  \n",
      "2            -6300000.0  9.410000e+07  2013     5513  \n",
      "3                   NaN  1.140000e+09  2013     6201  \n",
      "4                   NaN  3.121000e+09  2013     7084  \n",
      "..                  ...           ...   ...      ...  \n",
      "414                 NaN  2.899000e+09  2019  1688568  \n",
      "415                 NaN  2.206330e+08  2019  1757898  \n",
      "416                 NaN  4.290000e+08  2019    56873  \n",
      "417                 NaN  1.791000e+09  2019   849399  \n",
      "418                 NaN  4.140000e+09  2020  1613103  \n",
      "\n",
      "[7444 rows x 21 columns]\n",
      "printing df to write to csv\n",
      "         cik ebitda2013  ebitda2014  ebitda2015  ebitda2016  ebitda2017  \\\n",
      "0       1800          0           0           0           0           0   \n",
      "1       2488   3.39e+08     4.8e+07   -3.14e+08    1.33e+08    3.48e+08   \n",
      "2       2969        NaN  2.2851e+09  2.6355e+09  3.0319e+09  2.2934e+09   \n",
      "3       4127        NaN   5.652e+08  1.0231e+09  1.1187e+09  1.2538e+09   \n",
      "4       4281  1.422e+09   1.372e+09    1.28e+09   1.132e+09    8.77e+08   \n",
      "..       ...        ...         ...         ...         ...         ...   \n",
      "488  1754301        NaN         NaN         NaN         NaN         NaN   \n",
      "489  1754301        NaN         NaN         NaN         NaN         NaN   \n",
      "490  1755672        NaN         NaN         NaN         NaN         NaN   \n",
      "491  1757898        NaN         NaN         NaN         NaN         NaN   \n",
      "492  1770450        NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "      ebitda2018   ebitda2019   fcfe2013    fcfe2014  ...     cash2019 ticker  \\\n",
      "0              0            0  9.089e+09   3.336e+09  ...     3.86e+09    abt   \n",
      "1       6.21e+08     8.53e+08   1.09e+08  -1.169e+09  ...    1.466e+09    amd   \n",
      "2     2.9363e+09   3.2272e+09        NaN  2.6474e+09  ...   2.2487e+09    apd   \n",
      "3     1.3193e+09     9.52e+08        NaN   5.451e+08  ...    8.513e+08   swks   \n",
      "4      1.901e+09    1.571e+09          0           0  ...    1.648e+09    hwm   \n",
      "..           ...          ...        ...         ...  ...          ...    ...   \n",
      "488          NaN     2.12e+08        NaN         NaN  ...    3.234e+09    fox   \n",
      "489          NaN     2.12e+08        NaN         NaN  ...    3.234e+09   foxa   \n",
      "490          NaN    1.599e+09        NaN         NaN  ...    1.764e+09   ctva   \n",
      "491  1.78332e+08  2.25921e+08        NaN         NaN  ...  2.20633e+08    ste   \n",
      "492          NaN      4.3e+08        NaN         NaN  ...     2.74e+09    xrx   \n",
      "\n",
      "    avg_price2013 avg_price2014 avg_price2015 avg_price2016 avg_price2017  \\\n",
      "0       30.865046     36.177461     41.761090     37.389035     45.955036   \n",
      "1        3.378571      3.662064      2.331746      5.254921     12.411673   \n",
      "2       74.559651     99.016036    114.534238    119.110206    135.804867   \n",
      "3       21.678093     43.898232     82.857883     66.311559     95.660652   \n",
      "4             NaN           NaN           NaN           NaN           NaN   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "488           NaN           NaN           NaN           NaN           NaN   \n",
      "489           NaN           NaN           NaN           NaN           NaN   \n",
      "490           NaN           NaN           NaN           NaN           NaN   \n",
      "491     38.724741     49.605464     63.724561     65.699793     77.171300   \n",
      "492     19.943915     26.831501     25.107301     22.047664     26.372722   \n",
      "\n",
      "    avg_price2018 avg_price2019 current_price  \n",
      "0       61.982924     79.069025            98  \n",
      "1       17.214502     29.941071            69  \n",
      "2      154.404748    202.869345           291  \n",
      "3       89.492518     82.209002           131  \n",
      "4             NaN     29.267266            15  \n",
      "..            ...           ...           ...  \n",
      "488           NaN     34.552656            26  \n",
      "489           NaN     34.986916            26  \n",
      "490           NaN     27.175148            28  \n",
      "491    102.022221    135.749429           156  \n",
      "492     25.452387     30.542016            15  \n",
      "\n",
      "[493 rows x 150 columns]\n",
      "finished processing data\n",
      "249.5960648059845\n"
     ]
    }
   ],
   "source": [
    "# process the data in chunks first, return ticker level dataset \n",
    "def init_process_data():\n",
    "    \n",
    "    processed_list = []\n",
    "    \n",
    "    chunksize = 10 ** 5\n",
    "    for chunk in pd.read_csv(\"total_finance_table.csv\", chunksize=chunksize):  \n",
    "        #for each chunk, get finance info on a company, yearly level\n",
    "        processed_list.append(parse_data(chunk))\n",
    "    #turn list of dataframes into one dataframe\n",
    "    processed_table = pd.concat(processed_list)\n",
    "    \n",
    "    #split id back into year and cik\n",
    "    processed_table[['year','cik']] = processed_table.id.str.split(pat = \"_\", expand=True) \n",
    "    print(\"printing pre-aggregated data\")\n",
    "    print(processed_table)\n",
    "    del processed_list\n",
    "    #######################################################################################################\n",
    "    #aggregate up across chunks \n",
    "    agg_df = sqldf(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "    *,\n",
    "    IFNULL(operating_income,0) + IFNULL(dep_amort,0) AS ebitda, \n",
    "    IFNULL(cfo,0) - (IFNULL(ppe,0) - IFNULL(ppe_prev,0)) + IFNULL(lt_debt_proceeds,0) + IFNULL(st_debt_proceeds,0) - \n",
    "        IFNULL(lt_debt_repayments,0) - IFNULL(st_debt_repayments,0) AS fcfe\n",
    "    FROM \n",
    "    (\n",
    "        SELECT  \n",
    "        year,\n",
    "        CAST(cik AS INT64) as cik,\n",
    "        MAX(revenue) AS revenue, \n",
    "        MAX(cfo) AS cfo, \n",
    "        MAX(shares_outstanding) AS shares_outstanding, \n",
    "        MAX(current_assets) AS current_assets,\n",
    "        MAX(current_liabilities) AS current_liabilities,\n",
    "        MAX(dep_amort) AS dep_amort,\n",
    "        MAX(dep_only) AS dep_only,\n",
    "        MAX(amort_only) AS amort_only,\n",
    "        MAX(ppe) AS ppe,\n",
    "        MAX(ppe_prev) AS ppe_prev,\n",
    "        MAX(operating_income) AS operating_income,\n",
    "        MAX(lt_debt) AS lt_debt,\n",
    "        MAX(st_debt) AS st_debt,\n",
    "        MAX(lt_debt_proceeds) AS lt_debt_proceeds,\n",
    "        MAX(st_debt_proceeds) AS st_debt_proceeds,\n",
    "        MAX(lt_debt_repayments) AS lt_debt_repayments,\n",
    "        MAX(st_debt_repayments) AS st_debt_repayments,\n",
    "        MAX(cash) AS cash\n",
    "        FROM \n",
    "        processed_table \n",
    "        GROUP BY     \n",
    "        year,\n",
    "        cik\n",
    "        ORDER BY 2 DESC\n",
    "    )\n",
    "\n",
    "    \"\"\")\n",
    "    \n",
    "    del processed_table \n",
    "    #print(agg_df.columns)\n",
    "    \n",
    "    #######################################################################################################\n",
    "    # turn long data wide \n",
    "    wide_df = agg_df.pivot(index='cik', columns='year', values=[ 'ebitda', 'fcfe', \n",
    "                                                                'revenue','cfo', 'shares_outstanding', 'current_assets',\n",
    "                                                                'current_liabilities', 'dep_amort', 'dep_only', 'amort_only', \n",
    "                                                                'ppe','ppe_prev', 'operating_income', \n",
    "                                                                'lt_debt', 'st_debt',\n",
    "                                                                'lt_debt_proceeds', 'st_debt_proceeds', \n",
    "                                                                'lt_debt_repayments', 'st_debt_repayments', \n",
    "                                                                'cash'])\n",
    "    \n",
    "    del agg_df\n",
    "    \n",
    "    ####################################################################################################### \n",
    "    #fix columns\n",
    "    wide_df.columns = list(map(\"\".join, wide_df.columns))\n",
    "    \n",
    "    #add in tickers\n",
    "    wide_df = wide_df.merge(sample, on = ['cik'])\n",
    "    \n",
    "    #drop weird columns \n",
    "    wide_df = wide_df[wide_df.columns.drop(list(wide_df.filter(regex='1218')))]\n",
    "    wide_df = wide_df[wide_df.columns.drop(list(wide_df.filter(regex='2020')))]\n",
    "    \n",
    "    #read in easy stats csv\n",
    "    easy_stat_df = pd.read_csv(\"easy_stats_sp500.csv\")\n",
    "    \n",
    "    #join in easy stats\n",
    "    wide_df = wide_df.merge(easy_stat_df, on = 'ticker')\n",
    "    \n",
    "    ####################################################################################################### \n",
    "    \n",
    "    # add dataquality columns - \n",
    "    #WTK: how many revenue, ebitda, fcfe and shares_outstanding columns are available with 2019 info available\n",
    "    \n",
    "    # WTK: calculated estimated cagrs (FCFE growth, Revenue growth) and multiples (fcfe yield, EV/EBITDA)\n",
    "    \n",
    "    # forecast earnings, growth, multiple expansion or decrease over 10 years and get estimated IRR of stock\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################################### \n",
    "    #write to csv\n",
    "    print(\"printing df to write to csv\")\n",
    "    print(wide_df)\n",
    "    wide_df.to_csv(\"sp500_financials.csv\")\n",
    "    return(wide_df)\n",
    "    \n",
    "start = time.time()\n",
    "\n",
    "print(\"starting to process data\")\n",
    "init_df = init_process_data()\n",
    "\n",
    "end = time.time()\n",
    "print(\"finished processing data\")\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cik\n",
      "ebitda2013\n",
      "ebitda2014\n",
      "ebitda2015\n",
      "ebitda2016\n",
      "ebitda2017\n",
      "ebitda2018\n",
      "ebitda2019\n",
      "fcfe2013\n",
      "fcfe2014\n",
      "fcfe2015\n",
      "fcfe2016\n",
      "fcfe2017\n",
      "fcfe2018\n",
      "fcfe2019\n",
      "revenue2013\n",
      "revenue2014\n",
      "revenue2015\n",
      "revenue2016\n",
      "revenue2017\n",
      "revenue2018\n",
      "revenue2019\n",
      "cfo2013\n",
      "cfo2014\n",
      "cfo2015\n",
      "cfo2016\n",
      "cfo2017\n",
      "cfo2018\n",
      "cfo2019\n",
      "shares_outstanding2013\n",
      "shares_outstanding2014\n",
      "shares_outstanding2015\n",
      "shares_outstanding2016\n",
      "shares_outstanding2017\n",
      "shares_outstanding2018\n",
      "shares_outstanding2019\n",
      "current_assets2013\n",
      "current_assets2014\n",
      "current_assets2015\n",
      "current_assets2016\n",
      "current_assets2017\n",
      "current_assets2018\n",
      "current_assets2019\n",
      "current_liabilities2013\n",
      "current_liabilities2014\n",
      "current_liabilities2015\n",
      "current_liabilities2016\n",
      "current_liabilities2017\n",
      "current_liabilities2018\n",
      "current_liabilities2019\n",
      "dep_amort2013\n",
      "dep_amort2014\n",
      "dep_amort2015\n",
      "dep_amort2016\n",
      "dep_amort2017\n",
      "dep_amort2018\n",
      "dep_amort2019\n",
      "dep_only2013\n",
      "dep_only2014\n",
      "dep_only2015\n",
      "dep_only2016\n",
      "dep_only2017\n",
      "dep_only2018\n",
      "dep_only2019\n",
      "amort_only2013\n",
      "amort_only2014\n",
      "amort_only2015\n",
      "amort_only2016\n",
      "amort_only2017\n",
      "amort_only2018\n",
      "amort_only2019\n",
      "ppe2013\n",
      "ppe2014\n",
      "ppe2015\n",
      "ppe2016\n",
      "ppe2017\n",
      "ppe2018\n",
      "ppe2019\n",
      "ppe_prev2013\n",
      "ppe_prev2014\n",
      "ppe_prev2015\n",
      "ppe_prev2016\n",
      "ppe_prev2017\n",
      "ppe_prev2018\n",
      "ppe_prev2019\n",
      "operating_income2013\n",
      "operating_income2014\n",
      "operating_income2015\n",
      "operating_income2016\n",
      "operating_income2017\n",
      "operating_income2018\n",
      "operating_income2019\n",
      "lt_debt2013\n",
      "lt_debt2014\n",
      "lt_debt2015\n",
      "lt_debt2016\n",
      "lt_debt2017\n",
      "lt_debt2018\n",
      "lt_debt2019\n",
      "st_debt2013\n",
      "st_debt2014\n",
      "st_debt2015\n",
      "st_debt2016\n",
      "st_debt2017\n",
      "st_debt2018\n",
      "st_debt2019\n",
      "lt_debt_proceeds2013\n",
      "lt_debt_proceeds2014\n",
      "lt_debt_proceeds2015\n",
      "lt_debt_proceeds2016\n",
      "lt_debt_proceeds2017\n",
      "lt_debt_proceeds2018\n",
      "lt_debt_proceeds2019\n",
      "st_debt_proceeds2013\n",
      "st_debt_proceeds2014\n",
      "st_debt_proceeds2015\n",
      "st_debt_proceeds2016\n",
      "st_debt_proceeds2017\n",
      "st_debt_proceeds2018\n",
      "st_debt_proceeds2019\n",
      "lt_debt_repayments2013\n",
      "lt_debt_repayments2014\n",
      "lt_debt_repayments2015\n",
      "lt_debt_repayments2016\n",
      "lt_debt_repayments2017\n",
      "lt_debt_repayments2018\n",
      "lt_debt_repayments2019\n",
      "st_debt_repayments2013\n",
      "st_debt_repayments2014\n",
      "st_debt_repayments2015\n",
      "st_debt_repayments2016\n",
      "st_debt_repayments2017\n",
      "st_debt_repayments2018\n",
      "st_debt_repayments2019\n",
      "cash2013\n",
      "cash2014\n",
      "cash2015\n",
      "cash2016\n",
      "cash2017\n",
      "cash2018\n",
      "cash2019\n",
      "ticker\n",
      "avg_price2013\n",
      "avg_price2014\n",
      "avg_price2015\n",
      "avg_price2016\n",
      "avg_price2017\n",
      "avg_price2018\n",
      "avg_price2019\n",
      "current_price\n"
     ]
    }
   ],
   "source": [
    "for i in init_df.columns: print(i)\n",
    "#print(init_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to debug this dataset need tag, plabel, qtrs, stmt and hits descending\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ticker\n",
      "0      amd\n",
      "1     swks\n",
      "2      hes\n",
      "3      aep\n",
      "4      aal\n",
      "..     ...\n",
      "192    khc\n",
      "193     ir\n",
      "194    bkr\n",
      "195   avgo\n",
      "196   amcr\n",
      "\n",
      "[197 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#assess quality of data here \n",
    "pt_good = sqldf(\"\"\"SELECT DISTINCT ticker FROM init_df \n",
    "                    WHERE operating_income2019 > 0\n",
    "                    AND ppe2019 IS NOT NULL\n",
    "                    AND ppe_prev2019 IS NOT NULL\n",
    "                \"\"\")\n",
    "print(pt_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get other ticker info from other script (current price, dividend yield, market cap, shares outsanding , sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 2018 walmart ticker = wmt, fy = 2018\n",
    "# analyze missing info \n",
    "dq_df = pd.read_csv(\"total_finance_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = dq_df[(dq_df['ticker'] == 'dis')\n",
    "           #& (dq_df['fy'] == 2015.0)\n",
    "           \n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in ex.iterrows():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
