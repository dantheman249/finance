{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first job returns \n",
    "\n",
    "#import packages\n",
    "from datetime import datetime\n",
    "import lxml\n",
    "from lxml import html, etree\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from time import sleep\n",
    "import string\n",
    "import yfinance as yf\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import io\n",
    "import os\n",
    "import pandasql\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first make a function that will make the process of building a url easy.\n",
    "def make_url(base_url , comp):\n",
    "    \n",
    "    url = base_url\n",
    "    \n",
    "    # add each component to the base url\n",
    "    for r in comp:\n",
    "        url = '{}/{}'.format(url, r)\n",
    "        \n",
    "    return url\n",
    "\n",
    "# function to get unique values \n",
    "def unique(list1): \n",
    "    x = np.array(list1) \n",
    "    return(np.unique(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a list of 10-k urls  (summary files)\n",
    "def pull_10k_urls():\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"Starting to Pull All 10K URLs \")\n",
    "\n",
    "    list_10k = []\n",
    "\n",
    "    year_list = [ '2011','2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020']\n",
    "    for y in year_list: \n",
    "        \n",
    "        try: \n",
    "            print(\"Pulling Year: \"+y)\n",
    "            #get all 2019 10ks into dataframe of company, date, balance sheet, income statement, cash flow statement\n",
    "\n",
    "            # PULL DAILY INDEX FILINGS \n",
    "            base_url = r\"https://www.sec.gov/Archives/edgar/daily-index\"\n",
    "\n",
    "            # The daily-index filings, require a year and content type (html, json, or xml).\n",
    "            year_url = make_url(base_url, [y, 'index.json'])\n",
    "\n",
    "            # Display the new Year URL\n",
    "            # print('-'*100)\n",
    "            print('Building the URL for Year: {}'.format(y))\n",
    "            print(\"URL Link: \" + year_url)\n",
    "\n",
    "            # request the content for 2019, remember that a JSON strucutre will be sent back so we need to decode it.\n",
    "            content = requests.get(year_url)\n",
    "            decoded_content = content.json()\n",
    "\n",
    "            # def get_10k():\n",
    "            for item in decoded_content['directory']['item']:\n",
    "\n",
    "                # get the name of the folder\n",
    "                print('-'*100)\n",
    "                print('Pulling url for Quarter: {}'.format(item['name']))\n",
    "\n",
    "                # The daily-index filings, require a year, a quarter and a content type (html, json, or xml).\n",
    "                qtr_url = make_url(base_url, [y, item['name'], 'index.json'])\n",
    "\n",
    "                # print out the url.\n",
    "                print(\"URL Link: \" + qtr_url)\n",
    "\n",
    "                # Request, the new url and again it will be a JSON structure.\n",
    "                file_content = requests.get(qtr_url)\n",
    "                sleep(0.2)\n",
    "                decoded_content = file_content.json()\n",
    "\n",
    "                print('-'*100)\n",
    "                print('Pulling files')\n",
    "\n",
    "                #get just the master file urls into a df \n",
    "                master_url_list = []\n",
    "                for file in decoded_content['directory']['item']:\n",
    "                    if \"master\" in file['name']:\n",
    "                        file_url = make_url(base_url, [y, item['name'], file['name']]).replace(\".gz\",\"\")\n",
    "\n",
    "                        master_url_list.append(file_url)\n",
    "                        #print(\"File URL Link: \" + file_url)\n",
    "                        #sleep(2)\n",
    "                    else:\n",
    "                        pass\n",
    "                #print(master_url_df) \n",
    "\n",
    "                #traverse the master url files \n",
    "                for u in master_url_list: \n",
    "                    #get content of the file \n",
    "                    print(\"Requesting: \" + u)\n",
    "                    content = requests.get(u).content\n",
    "                    sleep(0.2)  \n",
    "\n",
    "                    try: \n",
    "                        data = content.decode(\"utf-8\").split('  ')\n",
    "                        # We need to remove the headers, so look for the end of the header and grab it's index\n",
    "                        for index, item in enumerate(data):\n",
    "                            if \"ftp://ftp.sec.gov/edgar/\" in item:\n",
    "                                start_ind = index\n",
    "                        data_format = data[start_ind + 1:]\n",
    "                        #list to store master data info \n",
    "                        master_data = []\n",
    "                        # now we need to break the data into sections, this way we can move to the final step of getting each row value.\n",
    "                        for index, item in enumerate(data_format):\n",
    "\n",
    "                            # if it's the first index, it won't be even so treat it differently\n",
    "                            if index == 0:\n",
    "                                clean_item_data = item.replace('\\n','|').split('|')\n",
    "                                clean_item_data = clean_item_data[8:]\n",
    "                            else:\n",
    "                                clean_item_data = item.replace('\\n','|').split('|')\n",
    "\n",
    "                            for index, row in enumerate(clean_item_data):\n",
    "\n",
    "                                # when you find the text file.\n",
    "                                if '.txt' in row:\n",
    "\n",
    "                                    # grab the values that belong to that row. It's 4 values before and one after.\n",
    "                                    mini_list = clean_item_data[(index - 4): index + 1]\n",
    "\n",
    "                                    if len(mini_list) != 0:\n",
    "                                        mini_list[4] = \"https://www.sec.gov/Archives/\" + mini_list[4]\n",
    "                                        master_data.append(mini_list)\n",
    "        #                                 print(mini_list)\n",
    "                                #print(master_data)\n",
    "\n",
    "                            master_data = pd.DataFrame.from_records(master_data, columns = ['cik', 'name', 'form','date', 'url'])\n",
    "                           # print(master_data)\n",
    "                            df_10k = master_data[master_data['form'] == \"10-K\"]\n",
    "                            #print(len(df_10k))\n",
    "                            if len(df_10k) > 0:\n",
    "                                list_10k.append(df_10k)\n",
    "                                print(df_10k)\n",
    "                            #print(list_10k)\n",
    "                    except Exception as ex: \n",
    "                        print(\"url failed: \"+ u)\n",
    "                        pass\n",
    "        except Exception as ex:\n",
    "            print (\"something broke\")\n",
    "\n",
    "    #End Code\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    print(\"Finished Pulling 10K URLs\")\n",
    "    df_10k = pd.concat(url_list_10k)\n",
    "    df_10k = df_10k.drop_duplicates()\n",
    "    \n",
    "    #write 10k links to csv\n",
    "    df_10k.to_csv(\"df_10k.csv\")\n",
    "    \n",
    "    #read in 10k df \n",
    "    df_10k_extract = pd.read_csv(\"df_10k.csv\")\n",
    "\n",
    "    #iterate through 10k urls and return df of urls that lead to filing summary \n",
    "    print(\"Start Pulling Filing Summaries\")\n",
    "    start = time.time()\n",
    "    summary_url_df = []\n",
    "\n",
    "    base_url = r\"https://www.sec.gov\"\n",
    "\n",
    "    for index, row in df_10k_extract.iterrows():\n",
    "        #print(row)\n",
    "        doc_url = row['url'].replace('-','').replace('.txt','/index.json')\n",
    "        print(\"Requesting: \"+doc_url)\n",
    "        content = requests.get(doc_url).json()\n",
    "        #rate limit of 10/second\n",
    "        sleep(0.2)\n",
    "\n",
    "        for file in content['directory']['item']:\n",
    "            # Grab the filing summary and create a new url leading to the file so we can download it.\n",
    "            if file['name'] == 'FilingSummary.xml':\n",
    "                xml_summary = base_url + content['directory']['name'] + \"/\" + file['name']\n",
    "                new_row = [row['cik'], row['name'], row['date'],  xml_summary]\n",
    "                summary_url_df.append(new_row)\n",
    "\n",
    "    summary_url_df = pd.DataFrame.from_records(summary_url_df, columns = ['cik', 'name', 'date', 'xml_summary'])\n",
    "    #print(summary_url_df['xml_summary'][0])\n",
    "\n",
    "    #write summary urls to a csv\n",
    "    summary_url_df.to_csv(\"summary_urls.csv\")\n",
    "    \n",
    "    #read in summary urls from csv \n",
    "    summary_url_extract = pd.read_csv(\"summary_urls.csv\")\n",
    "\n",
    "    #End Code\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    print(\"Finished Pulling Filing Summaries\")\n",
    "    \n",
    "    return(summary_url_extract)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
